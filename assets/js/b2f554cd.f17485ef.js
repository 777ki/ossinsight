"use strict";(self.webpackChunkdocus=self.webpackChunkdocus||[]).push([[1477],{30010:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"/explore-deep-in-4.6-billion-github-events","metadata":{"permalink":"/blog/explore-deep-in-4.6-billion-github-events","editUrl":"https://github.com/pingcap/ossinsight/edit/main/blog/explore-deep-in-4.6-billion-github-events.md","source":"@site/blog/explore-deep-in-4.6-billion-github-events.md","title":"Explore Deep in 4.6 Billion GitHub Events","description":"4.6 billion is literally an astronomical figure. The richest star map of our galaxy, brought by Gaia space observatory, includes just under 2 billion stars. What does a view of 4.6 billion GitHub events really look like? What secrets and values can be discovered in such an enormous amount of data?","date":"2022-05-03T00:00:00.000Z","formattedDate":"May 3, 2022","tags":[],"readingTime":9.55,"truncated":true,"authors":[{"name":"Fendy Feng","title":"Technical Content Developer","url":"https://github.com/septemberfd","imageURL":"https://github.com/septemberfd.png","key":"fendy"}],"nextItem":{"title":"Why We Choose TiDB to Support OSS Insight","permalink":"/blog/why-we-choose-tidb-to-support-oss-insight"}},"content":"4.6 billion is literally an astronomical figure. The richest star map of our galaxy, brought by Gaia space observatory, includes just under 2 billion stars. What does a view of 4.6 billion GitHub events really look like? What secrets and values can be discovered in such an enormous amount of data? \\n\\nHere you go: [OSSInsight.io](https://ossinsight.io/)** can help you find the answer**. It\u2019s a useful insight tool that can give you the most updated open source intelligence, and help you deeply understand any single GitHub project or quickly compare any two projects by digging deep into 4.6 billion GitHub events in real time. Here are some ways you can play with it.\\n\\n## Compare any two GitHub projects\\n\\nDo you wonder how different projects have performed and developed over time? Which project is worthy of more attention? **[OSSInsight.io](https://ossinsight.io/)** can answer your questions via the [Compare Projects](https://ossinsight.io/analyze/pingcap/tidb) page.\\n\\nLet\u2019s take the [Kubernetes repository](https://github.com/kubernetes/kubernetes)  (K8s) and Docker\u2019s [Moby repository](https://github.com/moby/moby) as examples and compare them in terms of popularity and coding vitality. \\n\\n\\n### **Popularity**\\n\\nTo compare the popularity of two repositories, we use multiple metrics including the number of stars, the growth trend of stars over time, and stargazers\u2019 geographic and employment distribution. \\n\\n#### **Number of stars**\\n\\nThe line chart below shows the accumulated number of stars of K8s and Moby each year. According to the chart, Moby was ahead of K8s until late 2019. The star growth of Moby slowed after 2017 while K8s has kept a steady growth pace. \\n\\n![](./media/the-star-history.png)\\n\\n<center><em>The star history of K8s and Moby</em></center>\\n\\n\\n#### **Geographical distribution of stargazers**\\n\\nThe map below shows the stargazers\u2019 geographical distribution of Moby and K8s. As you can see, their stargazers are scattered around the world with the majority coming from the US, Europe, and China.\\n\\n![](./media/geographicla-distribution-of-stargazers.png)\\n\\n\\n<center><em>The geographical distribution of K8s and Moby stargazers</em></center>\\n\\n#### **Employment distribution of stargazers**\\n\\nThe chart below shows the stargazers\u2019 employment of K8s (red) and Moby (dark blue). Both of their stargazers work in a wide range of industries, and most come from leading dotcom companies such as Google, Tencent, and Microsoft. The difference is that the top two companies of K8s\u2019 stargazers are  Google and Microsoft from the US, while Moby\u2019s top two followers are Tencent and Alibaba from China.  \\n\\n![](./media/employment-distribution-of-stargazers.png)\\n\\n\\n<center><em>The employment distribution of K8s and Moby stargazers</em></center>\\n\\n\x3c!--truncate--\x3e\\n\\n### **Coding vitality**\\n\\nTo compare the coding vitality of two GitHub projects, we use many metrics including the growth trend of pull requests (PRs), the monthly number of PRs, commits and pushes, and the heat map of developers\u2019 contribution time.  \\n\\n#### **Number of commits and pushes**\\n\\nThe bar chart below shows the number of commits and pushes submitted to K8s (top) and Moby (bottom) each month after their inception. Generally speaking, K8s has more pushes and commits than Moby, and their number grew stably until 2020 followed by a slowdown afterwards. Moby\u2019s monthly pushes and commits had a minor growth between 2015 and 2017, and then barely increased after 2018.\\n\\n![](./media/monthly-pushes-and-commits.png)\\n\\n<center><em>The monthly pushes and commits of K8s (top) and Moby (bottom)</em></center>\\n\\n#### **Number of PRs**\\n\\nThe charts below show the monthly and accumulated number of PRs of the two repositories. As you can see, K8s has received stable and consistent PR contributions ever since its inception and its accumulated number of PRs has also grown steadily. Moby had vibrant PR submissions before late 2017, but started to drop afterwards. Its accumulated number of PRs reached a plateau in 2017, which has remained the case ever since. \\n\\n![](./media/monthly-and-accumulated-pr-number.png)\\n\\n\\n<center><em>The monthly and accumulated PR number of K8s (top) and Moby (bottom)</em></center>\\n\\n#### **Developers\u2019 contribution time**\\n\\nThe following heat map shows developers\u2019 contribution time for K8s (left) and Moby (right). Each square represents one hour in a day. The darker the color, the more contributions occur during that time. K8s has many more dark parts than Moby, and K8s\u2019 contributions occur almost 24 hours a day, 7 days a week. K8s definitely has more dynamic coding activities than Moby. \\n\\n![](./media/heat-map.png)\\n\\n\\n<center><em>Heat map of developers\u2019 contribution time of K8s (left) and Moby (right)</em></center>\\n\\n<br />\\n\\n**Taken together**, these metrics show that while both K8s and Moby are popular across industries world-wide, K8s has more vibrant coding activities than Moby. K8s is continuously gaining popularity and coding vitality while Moby is falling in both over time.\\n\\nPopularity and coding vitality are just two dimensions to compare repositories. If you want to discover more insights or compare other projects you are interested in, feel free to visit the [Compare](https://ossinsight.io/analyze/pingcap/tidb) page and explore it for yourself.  \\n\\nOf course, you can use this same page to **deeply explore any single GitHub project** and gain the most up-to-date insights about them. The key metrics and the corresponding changes are presented in a panoramic view. More in-depth analytics such as code changes by PR size groups and PR lines are also available. Explore it for yourself and you\u2019d be surprised. Have fun. \\n\\n![](./media/panoramic-view-of-key-github-metrics.png)\\n\\n<center><em>Panoramic view of key GitHub metrics (K8s as an example)</em></center>\\n\\n<br />\\n\\n![](./media/total-pr-number-each-month-and-pr-groups.png)\\n\\n<center><em>  Total PR number each month/PR groups (K8s as an example)</em></center>\\n\\n<br />\\n\\n![](./media/number-of-lines-of-code-change-each-month.png)\\n\\n<center><em>The number of lines of code change each month (K8s as an example)</em></center>\\n\\n\\n## Key open source insights\\n\\n[OSSInsight.io](https://ossinsight.io/) does more than explore or compare repositories. **It gives you [historical, real-time, and custom open source insights](https://ossinsight.io/database/deep-insight-into-open-source-databases).** In this section, we\u2019ll share some key insights in open source databases and programming languages. If you want to gain insights in other areas, you can explore the [Insights](https://ossinsight.io/database/deep-insight-into-open-source-databases/)  page for yourself. \\n\\n**Note**: If you want to get those analytical results by yourself, you can execute the SQL commands above each chart on TiDB Cloud with ease following this [5-minute tutorial](https://ossinsight.io/blog/try-it-yourself/). \\n\\n\\n### **Rust: the most active programming language**\\n\\nRust was first released in 2012 and has been among the leading programming languages for 10 years. It has the most active repository with a total of 103,047 PRs at the time of writing. \\n\\n<details><summary>Click here to show SQL commands</summary>\\n<p>\\n\\n```sql\\nSELECT\\n    /*+ read_from_storage(tiflash[github_events]), MAX_EXECUTION_TIME(120000) */\\n    programming_language_repos.name AS repo_name,\\n    COUNT(*)      AS num\\nFROM github_events\\n         JOIN programming_language_repos ON programming_language_repos.id = github_events.repo_id\\nWHERE type = \'PullRequestEvent\'\\n  AND action = \'opened\'\\nGROUP BY 1\\nORDER BY 2 DESC\\nLIMIT 10\\n```\\n\\n</p>\\n</details>\\n\\n![](./media/pr-number-of-pl-repos.png)\\n\\n\\n<center><em>PR numbers of the leading programming languages</em></center>\\n\\n### **Go: the new favorite and the fastest growing programming language**\\n\\nAccording to **[OSSInsight.io](https://ossinsight.io/)**, 10 programming languages dominate the open source community. Go is the most popular with 108,317 stars, followed by Node and TypeScript. Go is also the fastest growing language in popularity.\\n\\n<details><summary>Click here to show SQL commands</summary>\\n<p>\\n\\n```sql\\nWITH repo_stars AS (\\n    SELECT\\n        /*+ read_from_storage(tiflash[github_events]) */\\n        repo_id,\\n        ANY_VALUE(repos.name) AS repo_name,\\n        COUNT(distinct actor_login) AS stars\\n    FROM github_events\\n         JOIN programming_language_repos repos ON repos.id = github_events.repo_id\\n    WHERE type = \'WatchEvent\'\\n    GROUP BY 1\\n), top_10_repos AS (\\n    SELECT\\n        repo_id, repo_name, stars\\n    FROM repo_stars rs\\n    ORDER BY stars DESC\\n    LIMIT 10\\n), tmp AS (\\n    SELECT\\n        /*+ read_from_storage(tiflash[github_events]) */\\n        event_year,\\n        tr.repo_name AS repo_name,\\n        COUNT(*) AS year_stars\\n    FROM github_events\\n         JOIN top_10_repos tr ON tr.repo_id = github_events.repo_id\\n    WHERE type = \'WatchEvent\' AND event_year <= 2021\\n    GROUP BY 2, 1\\n    ORDER BY 1 ASC, 2\\n), tmp1 AS (\\n    SELECT\\n        event_year,\\n        repo_name,\\n        SUM(year_stars) OVER(partition by repo_name order by event_year ASC) as stars\\n    FROM tmp\\n    ORDER BY event_year ASC, repo_name\\n)\\nSELECT event_year, repo_name, stars FROM tmp1\\n```\\n\\n</p>\\n</details>\\n\\n![](./media/star-growth-trends-of-leading-programming-languages.png)\\n\\n<center><em>The star growth trends of leading programming languages</em></center>\\n\\n\\n### **Microsoft and Google: the top two programing languages contributors**\\n\\nAs world-renowned high-tech companies, Microsoft and Google take the lead in open source language contributions with a total of 1,443 and 947 contributors respectively at the time of writing. \\n\\n<details><summary>Click here to show SQL commands</summary>\\n<p>\\n\\n```sql\\nSELECT\\n    /*+ read_from_storage(tiflash[github_events]), MAX_EXECUTION_TIME(120000) */\\n    TRIM(LOWER(REPLACE(u.company, \'@\', \'\'))) AS company,\\n    COUNT(DISTINCT actor_id)                 AS num\\nFROM\\n    github_events github_events\\n    JOIN programming_language_repos db ON db.id = github_events.repo_id\\n    JOIN users u ON u.login = github_events.actor_login\\nWHERE\\n    github_events.type IN (\\n        \'IssuesEvent\', \'PullRequestEvent\',\'IssueCommentEvent\',\\n        \'PullRequestReviewCommentEvent\', \'CommitCommentEvent\',\\n        \'PullRequestReviewEvent\'\\n    )\\n    AND u.company IS NOT NULL\\n    AND u.company != \'\'\\n    AND u.company != \'none\'\\nGROUP BY 1\\nORDER BY 2 DESC\\nLIMIT 20;\\n```\\n\\n</p>\\n</details>\\n\\n![](./media/companies-who-contribute-the-most-to-programing-languages.png)\\n\\n\\n<center><em>Companies who contribute the most to programing languages</em></center>\\n\\n\\n### **Elasticsearch draws the most attention**\\n\\nElasticsearch was one of the first open source databases. It is the most liked database with 64,554 stars, followed by Redis and Prometheus. From 2011 to 2016, Elasticseasrch and Redis shared the top spot until Elasticsearch broke away in 2017.\\n\\n<details><summary>Click here to show SQL commands</summary>\\n<p>\\n\\n```sql\\nWITH repo_stars AS (\\n    SELECT\\n        /*+ read_from_storage(tiflash[github_events]) */\\n        repo_id,\\n        ANY_VALUE(repos.name) AS repo_name,\\n        COUNT(distinct actor_login) AS stars\\n    FROM github_events\\n         JOIN db_repos repos ON repos.id = github_events.repo_id\\n    WHERE type = \'WatchEvent\'\\n    GROUP BY 1\\n), top_10_repos AS (\\n    SELECT\\n        repo_id, repo_name, stars\\n    FROM repo_stars rs\\n    ORDER BY stars DESC\\n    LIMIT 10\\n), tmp AS (\\n    SELECT\\n        /*+ read_from_storage(tiflash[github_events]) */\\n        event_year,\\n        tr.repo_name AS repo_name,\\n        COUNT(*) AS year_stars\\n    FROM github_events\\n         JOIN top_10_repos tr ON tr.repo_id = github_events.repo_id\\n    WHERE type = \'WatchEvent\' AND event_year <= 2021\\n    GROUP BY 2, 1\\n    ORDER BY 1 ASC, 2\\n), tmp1 AS (\\n    SELECT\\n        event_year,\\n        repo_name,\\n        SUM(year_stars) OVER(partition by repo_name order by event_year ASC) as stars\\n    FROM tmp\\n    ORDER BY event_year ASC, repo_name\\n)\\nSELECT event_year, repo_name, stars FROM tmp1\\n```\\n\\n</p>\\n</details>\\n\\n\\n![](./media/star-growth-trend-of-leading-databases.png)\\n\\n\\n<center><em>The star growth trend of leading databases</em></center>\\n\\n\\n### **China: the number one fan of open source databases**\\n\\nChina has the most open source database followers with 11,171 stargazers of database repositories, followed by the US and Europe. \\n\\n<details><summary>Click here to show SQL commands</summary>\\n<p>\\n\\n```sql\\nselect upper(u.country_code) as country_or_area, count(*) as count, count(*) / max(s.total) as percentage\\nfrom github_events\\nuse index(index_github_events_on_repo_id)\\nleft join users u ON github_events.actor_login = u.login\\njoin (\\n    -- Get the number of people has the country code.\\n    select count(*) as total\\n    from github_events\\n    use index(index_github_events_on_repo_id)\\n    left join users u ON github_events.actor_login = u.login\\n    where repo_id in (41986369, 48833910, 53311716) and github_events.type = \'WatchEvent\' and u.country_code is not null\\n) s\\nwhere repo_id in (41986369, 48833910, 53311716) and github_events.type = \'WatchEvent\' and u.country_code is not null\\ngroup by 1\\norder by 2 desc;\\n```\\n\\n</p>\\n</details>\\n\\n![](./media/geographical-distribution-database-stargazers.png)\\n\\n\\n<center><em>The geographical distribution of open source database stargazers</em></center>\\n\\n\\n### **CockroachDB gets the most feedback from the community of open source databases**\\n\\nIssues submitted by developers are important feedback from the community. CockroachDB has continually received issues during the past ten years and has ranked No.1 among all open source databases in the total number of issue submissions. \\n\\n<details><summary>Click here to show SQL commands</summary>\\n<p>\\n\\n```sql\\nSELECT\\n    /*+ read_from_storage(tiflash[github_events]) */\\n    db.group_name  AS repo_group_name,\\n    COUNT(distinct pr_or_issue_id) AS num\\nFROM\\n    github_events github_events\\n    JOIN osdb_repos db ON db.id = github_events.repo_id\\nWHERE type = \'IssuesEvent\'\\nGROUP BY 1\\nORDER BY 2 DESC\\n```\\n\\n</p>\\n</details>\\n\\n![](./media/issues-received-by-leading-open-source-databases.png)\\n\\n\\n<center><em>The number of issues received by leading open source databases</em></center>\\n<br />\\n\\n\\n**[OSSInsight.io](https://ossinsight.io/)** also allows you to create your own custom insights into any GitHub repository created after 2011. You\u2019re welcome to visit the [Insights page](https://ossinsight.io/database/deep-insight-into-open-source-databases) to explore more. \\n\\n\\n## Run your own analytics with TiDB Cloud\\n\\nAll the analytics on **[OSSInsight.io](https://ossinsight.io/)** are powered by [TiDB Cloud](https://en.pingcap.com/tidb-cloud/), a fully-managed database as a service. If you want to run your own analytics and get your own insights, sign up for a TiDB Cloud account and try it for yourself with this [5-minute tutorial](https://ossinsight.io/blog/try-it-yourself/).\\n\\n## Contact us \\n\\nDo you find **[OSSInsight.io](https://ossinsight.io/)** useful and fun to work with? Do you have any question or feedback to share with us? Feel free to [file an issue](https://github.com/pingcap/ossinsight/issues/new) on GitHub or follow us on [Twitter](https://twitter.com/OSSInsight/) to get the latest information. You\u2019re also welcome to share this insight tool with your friends."},{"id":"/why-we-choose-tidb-to-support-oss-insight","metadata":{"permalink":"/blog/why-we-choose-tidb-to-support-oss-insight","editUrl":"https://github.com/pingcap/ossinsight/edit/main/blog/why-we-choose-tidb-to-support-oss-insight.md","source":"@site/blog/why-we-choose-tidb-to-support-oss-insight.md","title":"Why We Choose TiDB to Support OSS Insight","description":"Many times we hope that our data access becomes more real-time. It means different for various industries: for logistics, it means that resource allocation can be carried out with a faster frequency; for e-commerce, it means quicker adjustment on promotion strategies based on sales information; for finance, it leads to faster risk management and more timely stop losses.","date":"2022-05-02T00:00:00.000Z","formattedDate":"May 2, 2022","tags":[],"readingTime":4.485,"truncated":false,"authors":[{"name":"ilovesoup","title":"PMM of PingCAP","url":"https://github.com/ilovesoup","imageURL":"https://github.com/ilovesoup.png","key":"ilovesoup"}],"prevItem":{"title":"Explore Deep in 4.6 Billion GitHub Events","permalink":"/blog/explore-deep-in-4.6-billion-github-events"},"nextItem":{"title":"Real-time insights on large volume of email data for SaaS CRM","permalink":"/blog/real-time-insights-on-large-volume-of-email-data-for-saas-crm"}},"content":"Many times we hope that our data access becomes more real-time. It means different for various industries: for logistics, it means that resource allocation can be carried out with a faster frequency; for e-commerce, it means quicker adjustment on promotion strategies based on sales information; for finance, it leads to faster risk management and more timely stop losses.\\n\\n![](/img/try-it-yourself/scenerios.png)\\n\\nFor developers, it means realtime insights on topics like the latest and hottest projects in the community, the organizations that contribute the most, the programming languages that are used the most, etc.; or, you want to know more personal information, such as what projects are your friends contributing recently, who are contributors to [TiDB](https://github.com/pingcap/tidb/?utm_source=ossinsight) \'s most recent PR, etc. Moreover, you want all these in real time.\\n\\n![](/img/try-it-yourself/top5-languages.png)\\n\\nFortunately, the [GH Archive](https://www.gharchive.org/) provides you the basic data to answer these questions. All you need is a database supporting these query. So simple!\\n\\nIn fact, if you think about it carefully, you might find that not easy: you want the system to provide summary statistics of a large amount of data, such as the hottest language ranking in the figure above, as well as a large number of concurrent accesses to individual accounts. You might need two systems: one of them focuses on high-concurrency detailed data services, while the other requires insight reports based on a large amount of data. Regardless of Github insights, you might encounter similar problems in your daily work. For example, if you are building a operational gaming data service system, you might face the customer inquery request: \\"I just looted the sword of infinity, but I can\'t find it in my backpack now!\\" You need to quickly locate the loot data of that unlucky player from large volume of records to get a clue of the situation. Did he accidentally destroyed the sword? Is it ninjaed? Does the bored player simply want a chat with GMs? At the mean time, the operation team is also urging: \\"For the recently launched class Night Lord, please give me the latest damage statistics immediately. I suspect that the Dark Hammer skill is too imba and need a nerf immediately.\\"\\n\\n**These all require your database to achieve all together:**\\n1. **Looking for a needle in a haystack**\\n2. **Quick analytical insights for massive data**\\n3. **Realtime updates**\\n\\nIn the past, for massive detailed data services, you could choose NoSQL or data sharding. NoSQL is a popular choice for massive hot data storage, but its disadvantages are also quite obvious: you cannot use SQL to express complex semantics, and it also lacks a proper indexing mechanism to locate data through dimensions other than the primary key; at the meantime, data sharding is quite cumbersome. Scaling the cluster as well as desining the partition key takes you a lot of effort. Moreover, for analytical services , you might need to deploy a dedicated analytical database in addition, and take care of the real-time ETL pipeline. For a small number of mission critical applications, you just grit your teeth and do it. But for the increasingly complex needs of analytics and data services, is it worthy.\\n\\nIn the selection, you want a solution with the SQL capabilities of traditional databases, mature indexing mechanism, real-time reporting ability and scalability.\\n\\nYes, [TiDB](https://en.pingcap.com/?utm_source=ossinsight) has them all.\\n\\n:::note\\n\ud83d\udca1 You can read **[TiDB Documentation](https://docs.pingcap.com/tidb/stable/overview?utm_source=ossinsight)** to get more useful infomation.\\n:::\\n\\nAs an HTAP database, TiDB has both complete transactions support and high performance analytics at ease. When users want to locate detailed data (such as querying the latest commit records via github ID or affiliated organization), you can build fine-grained indexes for multiple dimensions for fast locating and high concurrency; at the same time,  TiDB\'s analytics engine, [TiFlash](https://docs.pingcap.com/tidb/stable/tiflash-overview?utm_source=ossinsight) , has built-in support for high-frequency updates. A column storage system (yes, even transactional updates of hundreds of thousands of TPS) that allows data to be seamlessly synchronized in real-time for analysis. Users only need to submit SQL queries without caring about the types of query, and the optimizer will automatically choose the optimal way to evaluate. In fact, it may not be possible for you to precisely distinguish which engine is more suitable. Consider quering for an individual organization: if it is a small organization with three or five people, the Github events since its creation may not exceed 10,000; while for giants like Microsoft and Alibaba, the difference on data volume is in orders of magnitude. In such case, the choice of engines becomes quite subtle, let alone separate the workloads in different databases. For TiDB, the choice is automatic: through statistical estimation, TiDB can \\"guess\\" the amount of data accessed and combine it with a cost model to find the most appropriate way to execute.\\n\\nIn fact, in addition to interesting projects like OSS Insight, TiDB has a wide range of applications in similar real-time data insights and services, such as history order serving , advertising , risk management, datahub, logistics tracking and etc.\\n\\nUnleashing the value of real-time data has become more and more important, and hope [TiDB](https://docs.pingcap.com/tidb/stable/overview?utm_source=ossinsight) can lend you a helping hand.\\n\\n:::info\\n### \ud83c\udf1f Details in how OSS Insight works\\n\\nGo to read [Use TiDB Cloud to Analyze GitHub Events in 5 Minutes](/blog/try-it-yourself) and use the [Developer Tier](https://tidbcloud.com/signup?utm_source=ossinsight) **free** for 1 year.\\n\\nYou can find how we deal with massive github data in [Data Preparation for Analytics](/blog/how-it-works) as well!\\n:::"},{"id":"/real-time-insights-on-large-volume-of-email-data-for-saas-crm","metadata":{"permalink":"/blog/real-time-insights-on-large-volume-of-email-data-for-saas-crm","editUrl":"https://github.com/pingcap/ossinsight/edit/main/blog/real-time-insights-on-large-volume-of-email-data-for-saas-crm.md","source":"@site/blog/real-time-insights-on-large-volume-of-email-data-for-saas-crm.md","title":"Real-time insights on large volume of email data for SaaS CRM","description":"Providing insights on large volume of email data might not be as easy as we thought. While data coming in real-time, indices and metadata are to be built consistently. To make things worse, the data volume is beyond traditional single node databases\' reach.","date":"2022-05-01T00:00:00.000Z","formattedDate":"May 1, 2022","tags":[],"readingTime":2.72,"truncated":false,"authors":[{"name":"ilovesoup","title":"PMM of PingCAP","url":"https://github.com/ilovesoup","imageURL":"https://github.com/ilovesoup.png","key":"ilovesoup"}],"prevItem":{"title":"Why We Choose TiDB to Support OSS Insight","permalink":"/blog/why-we-choose-tidb-to-support-oss-insight"},"nextItem":{"title":"Use TiDB Cloud to Analyze GitHub Events in 5 Minutes","permalink":"/blog/try-it-yourself"}},"content":"> Providing insights on large volume of email data might not be as easy as we thought. While data coming in real-time, indices and metadata are to be built consistently. To make things worse, the data volume is beyond traditional single node databases\' reach.\\n\\n## Background\\n\\nTo store large volumes of real-time user data like email and provide insights is not easy. If your application is layered on top of Gmail to automatically extract and organize the useful information buried inside of our inboxes.\\n\\n It became clear that they were going to need a better system for organizing terabytes of email metadata to power collaboration as their customer base rapidly increased, it is not easy to provide insights. You need to organize email data by first applying a unique identifier to the emails and then proactively indexing the email metadata. The unique identifier is what connects the same email headers across. For each email inserted in real-time, the system extracts meta information from it and builds indices for high concurrent access. When data volume is small, it\'s all good: traditional databases provide all you need. However, when data size grows beyond a single node\'s capacity, everything becomes very hard.\\n\\n## Potential Database Solutions\\n\\nRegarding databases, there are some options you might consider:\\n\\n1. **NoSQL database.** While fairly scalable, it does not provide you indexing and comprehensive query abilities. You might end up implementing them in your application code.\\n2. **Sharing cluster of databases.** Designing sharding key and paying attention to the limitations between shards are painful. It might be fine for applications with simple schema designs, but it will be too complicated for CRM. Moreover, it\'s very hard to maintain.\\n3. **Analytical databases.** They are fine for dashboard and reporting. But not fine for high concurrent updates and index based serving.\\n\\n## How to get real-time insights\\n\\n[TiDB](https://docs.pingcap.com/tidb/stable/overview?utm_source=ossinsight) is a distributed database with user experience of traditional databases. It looks like a super large MySQL without the limitations of NoSQL and sharding cluster solutions. With TiDB, you can simply have the base information, indices and metadata being updated in a concurrent manner with the help of cross-node transaction ability. \\n\\nTo build such a system, you just need following steps:\\n\\n1. **Create schemas** according to your access pattern with indices on user name, organization, job title etc.\\n2. **Use streaming system** to gather and extract meta information from your base data\\n3. **Insert into TiDB via ordinary MySQL client driver like JDBC.** You might want to gather data in small batches of hundreds of rows to speed up ingestion. In a single transaction, updates on base data, indices and meta information are guaranteed to be consistent.\\n4. Optionally, **deploy a couple of [TiFlash](https://docs.pingcap.com/tidb/stable/tiflash-overview?utm_source=ossinsight) nodes** to speed up large scale reporting queries.\\n5. **Access the data** just like in MySQL and you are all done. SQL features for analytics like aggregations, multi-joins or window functions are all supported with great performance.\\n\\nFor more cases, please see [here](https://en.pingcap.com/customers/?utm_source=ossinsight).\\n\\n\\n\\n:::info\\n### \ud83c\udf1f Details in how OSS Insight works\\n\\nGo to read [Use TiDB Cloud to Analyze GitHub Events in 5 Minutes](/blog/try-it-yourself) and use the [Developer Tier](https://tidbcloud.com/signup?utm_source=ossinsight) **free** for 1 year.\\n\\nYou can find how we deal with massive github data in [Data Preparation for Analytics](/blog/how-it-works) as well!\\n:::"},{"id":"/try-it-yourself","metadata":{"permalink":"/blog/try-it-yourself","editUrl":"https://github.com/pingcap/ossinsight/edit/main/blog/try-it-yourself.md","source":"@site/blog/try-it-yourself.md","title":"Use TiDB Cloud to Analyze GitHub Events in 5 Minutes","description":"TiDB is an open source distributed NewSQL database with horizontal scalability, high availability, and strong consistency. It can also deal with mixed OLTP and OLAP workloads at the same time by leveraging its hybrid transactional and analytical (HTAP) capability.","date":"2022-04-01T00:00:00.000Z","formattedDate":"April 1, 2022","tags":[],"readingTime":5.065,"truncated":true,"authors":[{"name":"Fendy Feng","title":"Technical Content Developer","url":"https://github.com/septemberfd","imageURL":"https://github.com/septemberfd.png","key":"fendy"},{"name":"hooopo","title":"Engineer of TiDB Community","url":"https://twitter.com/hooopo","imageURL":"https://github.com/hooopo.png","key":"hooopo"}],"prevItem":{"title":"Real-time insights on large volume of email data for SaaS CRM","permalink":"/blog/real-time-insights-on-large-volume-of-email-data-for-saas-crm"},"nextItem":{"title":"Data Preparation for Analytics","permalink":"/blog/how-it-works"}},"content":"[TiDB](https://docs.pingcap.com/tidb/stable/overview?utm_source=ossinsight) is an open source distributed NewSQL database with horizontal scalability, high availability, and strong consistency. It can also deal with mixed OLTP and OLAP workloads at the same time by leveraging its hybrid transactional and analytical (HTAP) capability. \\n\\n**[TiDB Cloud](https://docs.pingcap.com/tidbcloud/public-preview?utm_source=ossinsight) is a fully-managed Database-as-a-Service (DBaaS)** that brings everything great about TiDB to your cloud and lets you focus on your applications, not the complexities of your database. \\n\\nIn this tutorial, we will provide you with a piece of sample data of all GitHub events occurring on January 1, 2022, and walk you through on how to use TiDB Cloud to analyze this data in 5 minutes.  \\n\\n## Sign up for a TiDB Cloud account (Free)\\n\\n1. Click [here](https://tidbcloud.com/signup?utm_source=ossinsight) to sign up for a TiDB Cloud account free of charge. \\n2. [Log in](https://tidbcloud.com/?utm_source=ossinsight) to your account.\\n\\n\x3c!--truncate--\x3e\\n\\n## Create a TiDB Developer Tier cluster (Free)\\nOnce you register an account, you can create a free cluster with TiDB Developer Tier. \\n\\n:::info\\n A cluster is a database to store data. \\n:::\\n\\n1. Click **Get Started for Free** and start to create a free cluster. \\n\\n![](/img/try-it-yourself/dev-tier.png)\\n\\n2. On the **Create a Cluster** page, set up your cluster name and root password.\\n3. Note that the cloud provider is AWS by default, and then **MUST** select the `US-West-2 (Oregon)` region to create the cluster.\\n4. The cluster tier is S1.dev by default.\\n5. Click **Submit**.\\nYour TiDB Cloud cluster will be created in approximately 1 to 3 minutes.\\n\\n:::note\\nThe Developer Tier is **free** for 1 year.\\n:::\\n\\n## Import data to your TiDB Cloud cluster\\n\\n### Import the data\\nOnce your cluster is ready, you can start to import the sample data to your cluster. \\n\\n:::info\\nWe have merged the create database/table in the SQL files, so you don\'t need to `create database/tables` by yourself.\\n\\nIf you want to know the table schema, you can check `desc gharchive_dev` later in the following step. \\n:::\\n\\n1. Click your cluster name in **Active Cluster** page to get into the detail page of your cluster.\\n2. Click the **Import** button on the **Active Clusters** page and then go to the **Data Import Task** page. \\n\\n![](/img/try-it-yourself/import.png)\\n\\n3. Copy the values below and paste to the blanks of **Bucket URL** and **Role-ARN** respectively on the **Data Import Task** page.\\n\\n**Bucket URL**:\\n```\\ns3://tidbcloud-samples/gharchive/\\n```\\n**Role-ARN**:\\n```\\narn:aws:iam::385595570414:role/import-sample-access\\n```\\n\\n4. Choose **US West (Oregon)** for your **Bucket region**;\\n5. Tick **TiDB Dumpling** for the **Data Format**. \\n6. Input your cluster password in the blank of **Password** on the **Target Database** section. \\n\\n![](/img/try-it-yourself/fill.png)\\n\\n7. After you fill in all the blanks on the **Data Import Task** page, click the **Import** button at the bottom of this page and wait for a few moments for the system to complete data importing. \\n\\n\\n### Use the web shell to check if data is ready\\nTiDB Cloud provides a web shell to connect the database online. \\n1. Click the **Exit** button after you successfully import the data into your cluster. \\n2. Click your cluster name in **Active Cluster** page to get into the detail page of your cluster.\\n3. Then, click the **Connect** button and the **Connect to TiDB** panel pops out. \\n4. Choose **Web SQL Shell** --\x3e **Open SQL Shell**. \\n5. Then input your cluster password as shown in the image below.\\n\\n![](/img/try-it-yourself/web-shell.png)\\n\\n\\n### Set column storage replica: TiFlash (Optional but coult make SQL faster!) \\n\\n[TiFlash](https://docs.pingcap.com/tidb/stable/tiflash-overview?utm_source=ossinsight) is the key component that makes TiDB / TiDB Cloud an HTAP database and capable of dealing with OLTP and OLAP workloads at the same time. \\n\\nHere, you can try the following SQL commands on TiDB Cloud to experience its real-time analytics with ease.\\n\\n1. Execute the SQL statements specified below \\n\\n```sql\\nuse gharchive_dev;\\nALTER TABLE github_events SET TIFLASH REPLICA 1;\\n```\\n\\n2. Setting a TiFlash replica will take you some time, so you can use the following SQL statements to check if the procedure is done or not. \\n\\n```sql\\nSELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = \'gharchive_dev\' and TABLE_NAME = \'github_events\';\\n```\\n\\nIf the results you get are the same as follows, then it means the procedure is done. \\n\\n```sql\\nmysql> SELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = \'gharchive_dev\' and TABLE_NAME = \'github_events\';\\n+---------------+---------------+----------+---------------+-----------------+-----------+----------+\\n| TABLE_SCHEMA  | TABLE_NAME    | TABLE_ID | REPLICA_COUNT | LOCATION_LABELS | AVAILABLE | PROGRESS |\\n+---------------+---------------+----------+---------------+-----------------+-----------+----------+\\n| gharchive_dev | github_events |       68 |             1 |                 |         1 |        1 |\\n+---------------+---------------+----------+---------------+-----------------+-----------+----------+\\n1 row in set (0.27 sec)\\n\\nmysql>\\n```\\n\\n## Analysis!\\n\\nAfter you finish all the steps above, you can start the analytical process. \\n\\n:::tip\\nIf you want to know the table schema, you can use `show create table tbl_name` to get that information.\\n:::\\n\\nBecause you have imported the sample data of all GitHub events occurred on the first hour of 2022 (from 2022-01-01 00:00:00 to 2022-01-01 00:59:59), you can start to make any queries based on that data by using SQL commands. \\n\\n### How many events occurred in total?\\nExecute the following SQL statement to query the total number of events. \\n\\n```sql\\nSELECT count(*) FROM github_events;\\n```\\n\\n### Which repository gets the most stars?\\nExecute the following statements to query the most starred repository. \\n\\n```sql\\n  SELECT repo_name, count(*) AS events_count\\n    FROM github_events\\n   WHERE type = \'WatchEvent\' /* Yes, `WatchEvent` means star */\\nGROUP BY 1\\nORDER BY 2 DESC\\n   LIMIT 20;\\n```\\n\\n\\n## Mini Test\\nHere is a small test for you to practice how to use TiDB Cloud to conduct analytics. \\n\\n### Q: Who is the most active contributor except the robot accounts on the first hour of 2022?\\n\\n### Click for the answer. \u2b07\ufe0f\\n\\n<details><summary>Click me to show answer</summary>\\n<p>\\n\\n```sql\\n  SELECT actor_login, \\n         count(*) AS events_count\\n    FROM github_events\\n   WHERE actor_login NOT LIKE \'%bot%\'\\nGROUP BY 1\\nORDER BY 2 DESC \\n   LIMIT 20\\n```\\n\\n</p>\\n</details>\\n\\n## Watch the video below for detailed information\\n\\n<video width=\\"100%\\" poster=\\"/img/try-it-yourself/dev-tier.png\\" controls>\\n  <source src=\\"/video/github-demo-tidbcloud.mp4\\" type=\\"video/mp4\\" />\\n</video>\\n\\n\\n:::info\\n### \ud83c\udf1f Details in how OSS Insight works\\n\\nFind the reason [Why We Choose TiDB to Support OSS Insight](/blog/why-we-choose-tidb-to-support-oss-insight).\\n\\nYou can find how we deal with massive github data in [Data Preparation for Analytics](/blog/how-it-works) as well!\\n:::"},{"id":"/how-it-works","metadata":{"permalink":"/blog/how-it-works","editUrl":"https://github.com/pingcap/ossinsight/edit/main/blog/how-it-works.md","source":"@site/blog/how-it-works.md","title":"Data Preparation for Analytics","description":"All the data we use here on this website sources from GH Archive, a non-profit project that records and archives all GitHub events data since 2011. The total data volume archived by GH Archive can be up to 4 billion rows. We download the json file on GH Archive and convert it into csv format via Script, and finally load it into the TiDB cluster in parallel through TiDB-Lightning.","date":"2022-03-01T00:00:00.000Z","formattedDate":"March 1, 2022","tags":[],"readingTime":4.55,"truncated":true,"authors":[{"name":"hooopo","title":"Engineer of TiDB Community","url":"https://twitter.com/hooopo","imageURL":"https://github.com/hooopo.png","key":"hooopo"}],"prevItem":{"title":"Use TiDB Cloud to Analyze GitHub Events in 5 Minutes","permalink":"/blog/try-it-yourself"}},"content":"All the data we use here on this website sources from [GH Archive](https://www.gharchive.org/), a non-profit project that records and archives all GitHub events data since 2011. The total data volume archived by GH Archive can be up to 4 billion rows. We download the `json file` on GH Archive and convert it into csv format via Script, and finally load it into the TiDB cluster in parallel through [TiDB-Lightning](https://docs.pingcap.com/tidb/stable/tidb-lightning-overview).\\n\\nIn this post, we will explain step by step how we conduct this process. \\n\\n1. Prepare the data in csv format for TiDB Lighting. \\n\\n\x3c!--truncate--\x3e\\n\\n```\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000000.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000001.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000002.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000003.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000004.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000005.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000006.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000007.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000008.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000009.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000010.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000011.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000012.csv\\n\u251c\u2500\u2500 gharchive_dev.github_events.000000000013.csv\\n```\\n\\n2. Configure the TiDB Lightning as follows.\\n\\n```\\ncat tidb-lightning.toml\\n[mydumper.csv]\\nseparator = \',\'\\ndelimiter = \'\\"\'\\nheader = true\\nnot-null = false\\nbackslash-escape = true\\ntrim-last-separator = false\\n\\n[tikv-importer]\\n backend = \\"local\\"\\n sorted-kv-dir = \\"/kvdir/\\"\\n\\ndisk-quota = \\"1.5TiB\\"\\n\\n[mydumper]\\ndata-source-dir = \\"/csv_dir/\\"\\nstrict-format = false\\nno-schema = true\\n\\n[tidb]\\nhost = \\"xxx\\"\\nport = 3306\\nuser = \\"github_events\\"\\npassword = \\"******\\"\\n\\n[lightning]\\ncheck-requirements = false\\nregion-concurrency = 32\\nmeta-schema-name = \\"gharchive_meta\\"\\n```\\n\\n3. Load the data into the TiDB cluster. \\n\\n```bash\\nnohup tidb-lightning -config ./tidb-lightning.toml > nohup.out\\n```\\n\\n4. Convert the unstructured `json file` provided by GH Archive into structured data. \\n\\n```\\ngharchive_dev> desc github_events;\\n+--------------------+--------------+------+-----+---------+-------+\\n| Field              | Type         | Null | Key | Default | Extra |\\n+--------------------+--------------+------+-----+---------+-------+\\n| id                 | bigint(20)   | YES  | MUL | <null>  |       |\\n| type               | varchar(255) | YES  | MUL | <null>  |       |\\n| created_at         | datetime     | YES  | MUL | <null>  |       |\\n| repo_id            | bigint(20)   | YES  | MUL | <null>  |       |\\n| repo_name          | varchar(255) | YES  | MUL | <null>  |       |\\n| actor_id           | bigint(20)   | YES  | MUL | <null>  |       |\\n| actor_login        | varchar(255) | YES  | MUL | <null>  |       |\\n| actor_location     | varchar(255) | YES  |     | <null>  |       |\\n| language           | varchar(255) | YES  | MUL | <null>  |       |\\n| additions          | bigint(20)   | YES  | MUL | <null>  |       |\\n| deletions          | bigint(20)   | YES  | MUL | <null>  |       |\\n| action             | varchar(255) | YES  | MUL | <null>  |       |\\n| number             | int(11)      | YES  |     | <null>  |       |\\n| commit_id          | varchar(255) | YES  | MUL | <null>  |       |\\n| comment_id         | bigint(20)   | YES  | MUL | <null>  |       |\\n| org_login          | varchar(255) | YES  | MUL | <null>  |       |\\n| org_id             | bigint(20)   | YES  | MUL | <null>  |       |\\n| state              | varchar(255) | YES  |     | <null>  |       |\\n| closed_at          | datetime     | YES  | MUL | <null>  |       |\\n| comments           | int(11)      | YES  | MUL | <null>  |       |\\n| pr_merged_at       | datetime     | YES  | MUL | <null>  |       |\\n| pr_merged          | tinyint(1)   | YES  |     | <null>  |       |\\n| pr_changed_files   | int(11)      | YES  | MUL | <null>  |       |\\n| pr_review_comments | int(11)      | YES  | MUL | <null>  |       |\\n| pr_or_issue_id     | bigint(20)   | YES  | MUL | <null>  |       |\\n| event_day          | date         | YES  | MUL | <null>  |       |\\n| event_month        | date         | YES  | MUL | <null>  |       |\\n| author_association | varchar(255) | YES  |     | <null>  |       |\\n| event_year         | int(11)      | YES  | MUL | <null>  |       |\\n| push_size          | int(11)      | YES  |     | <null>  |       |\\n| push_distinct_size | int(11)      | YES  |     | <null>  |       |\\n+--------------------+--------------+------+-----+---------+-------+\\n```\\n\\n5. With structured data at hand, we can start to make further analysis with TiDB Cloud. Execute SQL commands to generate analytical results. For example, you can execute SQL commands below to output the top 10 most starred JavaScript framework repos in 2021.\\n\\n```\\n  SELECT js.name, count(*) as stars \\n    FROM github_events \\n         JOIN js_framework_repos js ON js.id = github_events.repo_id \\n   WHERE type = \'WatchEvent\' and event_year = 2021 \\nGROUP BY 1 \\nORDER BY 2 DESC\\n   LIMIT 10;\\n+-------------------+-------+\\n| name              | stars |\\n+-------------------+-------+\\n| facebook/react    | 22830 |\\n| sveltejs/svelte   | 18573 |\\n| vuejs/vue         | 18015 |\\n| angular/angular   | 11037 |\\n| alpinejs/alpine   | 6993  |\\n| preactjs/preact   | 2965  |\\n| hotwired/stimulus | 1355  |\\n| marko-js/marko    | 1006  |\\n| neomjs/neo        | 826   |\\n| tastejs/todomvc   | 813   |\\n+-------------------+-------+\\n```\\n\\nWe have analyzed all the GitHub projects regarding databases, JavaScripe frameworks, programming languages, web frameworks, and low-code development tools, and provided valuable insights in 2021, in real time, and custom insights. If the repository you care about is not included here, you\'re welcome to submit your PR [here](https://github.com/hooopo/gharchive/tree/main/meta/repos). If you want to gain more insights into other areas, you can try TiDB Cloud by yourselves with this [5-minute tutorial](/blog/try-it-yourself/). \\n\\nBelow are the areas of GitHub projects we have analyzed. \\n\\n```\\ngharchive_dev> show tables;\\n+-----------------------------+\\n| Tables_in_gharchive_dev     |\\n+-----------------------------+\\n| cn_repos                    |\\n| css_framework_repos         |\\n| db_repos                    |\\n| github_events               |\\n| js_framework_repos          |\\n| nocode_repos                |\\n| programming_language_repos  |\\n| static_site_generator_repos |\\n| web_framework_repos         |\\n+-----------------------------+\\n```\\n\\n:::info\\n### \ud83c\udf1f Details in how OSS Insight works\\n\\nGo to read [Use TiDB Cloud to Analyze GitHub Events in 5 Minutes](/blog/try-it-yourself) and use the [Developer Tier](https://tidbcloud.com/signup?utm_source=ossinsight) **free** for 1 year.\\n\\nYou can find the reason [Why We Choose TiDB to Support OSS Insight](/blog/why-we-choose-tidb-to-support-oss-insight) as well!\\n:::"}]}')}}]);