{
    "version": "https://jsonfeed.org/version/1",
    "title": "Open Source Software Insight Blog",
    "home_page_url": "https://ossinsight.io/blog",
    "description": "Open Source Software Insight Blog",
    "items": [
        {
            "id": "/about",
            "content_html": "<h3>Who we are?</h3><blockquote><p>PingCAP started in 2015 when three seasoned infrastructure engineers were sick and tired of the way databases were managed, scaled, and maintained while working at leading Internet companies.</p><p>Seeing no good solution on the market, they decided to build one themselves — the open-source way.</p><p>With the help of a first-class team, and hundreds of contributors from around the globe, PingCAP is building an open-source distributed NewSQL Hybrid Transactional and Analytical Processing (HTAP) database. TiDB, our flagship project, is a cloud-native distributed SQL layer with MySQL compatibility, and one of the most popular open-source database projects in the world (don’t take our word for it, check it out). TiDB’s sister project, TiKV, is a cloud-native distributed Key-Value store. It is now a CNCF Graduated project.</p></blockquote><p><a href=\"https://pingcap.com/about-us/\">Learn more about PingCAP</a></p><h3>What is TiDB Cloud?</h3><blockquote><p>Get a fully managed service of TiDB: an open source, cloud native, distributed, MySQL compatible database for elastic scale and real-time analytics.</p><p>TiDB Cloud is now in Public Preview and offers a one-year free trial with the Developer Tier.</p></blockquote><h3>Why we build this project?</h3><blockquote><p><em>&quot;We love data, so we create TiDB, we love open source, so we analyze it.&quot;</em> - PingCAP</p></blockquote><h3>Where the data comes from?</h3><blockquote><p>Open-source developers all over the world are working on millions of projects: writing code &amp; documentation, fixing &amp; submitting bugs, and so forth. GH Archive is a project to record the public GitHub timeline, archive it, and make it easily accessible for further analysis.</p></blockquote><p><a href=\"http://www.gharchive.org/\">Learn more about GH Archive</a></p><h3>where is the data stored?</h3><blockquote><p>We use <a href=\"https://tidbcloud.com\">TiDB Cloud</a> to store data, more than 4 billion rows.</p></blockquote><h3>How is the instantaneity of the data?</h3><blockquote><p>OSS Insight project provides data hourly, even though TiDB Cloud can be realtime...</p></blockquote><h3>How many clusters ( databases ) are used?</h3><blockquote><p>OOONLY one, for both OLTP and OLAP queries.</p></blockquote><h3>Contact US</h3><blockquote><p><a href=\"mailto:community@tidb.io\">community@tidb.io</a></p></blockquote>",
            "url": "https://ossinsight.io/blog/about",
            "title": "About",
            "summary": "Who we are?",
            "date_modified": "2022-04-19T04:45:13.663Z",
            "author": {
                "name": "PingCAP",
                "url": "https://github.com/pingcap"
            }
        },
        {
            "id": "/changelog",
            "content_html": "<h2>2022-03-22</h2><ul><li>Add &quot;Compare Projects&quot; tools</li><li>Add &quot;Try It Yourself&quot; blog</li></ul>",
            "url": "https://ossinsight.io/blog/changelog",
            "title": "Changelog",
            "summary": "2022-03-22",
            "date_modified": "2022-04-19T04:45:13.663Z"
        },
        {
            "id": "/how-it-works",
            "content_html": "<p>All the data we use here on this website sources from <a href=\"https://www.gharchive.org/\">GH Archive</a>, a non-profit project that records and archives all GitHub events data since 2011. The total data volume archived by GH Archive can be up to 4 billion rows. We download the <code>json file</code> on GH Archive and convert it into csv format via Script, and finally load it into the TiDB cluster in parallel through <a href=\"https://docs.pingcap.com/tidb/stable/tidb-lightning-overview\">TiDB-Lightning</a>.</p><p>In this post, we will explain step by step how we conduct this process. </p><ol><li>Prepare the data in csv format for TiDB Lighting. </li></ol><pre><code>├── gharchive_dev.github_events.000000000000.csv\n├── gharchive_dev.github_events.000000000001.csv\n├── gharchive_dev.github_events.000000000002.csv\n├── gharchive_dev.github_events.000000000003.csv\n├── gharchive_dev.github_events.000000000004.csv\n├── gharchive_dev.github_events.000000000005.csv\n├── gharchive_dev.github_events.000000000006.csv\n├── gharchive_dev.github_events.000000000007.csv\n├── gharchive_dev.github_events.000000000008.csv\n├── gharchive_dev.github_events.000000000009.csv\n├── gharchive_dev.github_events.000000000010.csv\n├── gharchive_dev.github_events.000000000011.csv\n├── gharchive_dev.github_events.000000000012.csv\n├── gharchive_dev.github_events.000000000013.csv\n</code></pre><ol start=\"2\"><li>Configure the TiDB Lightning as follows.</li></ol><pre><code>cat tidb-lightning.toml\n[mydumper.csv]\nseparator = &#x27;,&#x27;\ndelimiter = &#x27;&quot;&#x27;\nheader = true\nnot-null = false\nbackslash-escape = true\ntrim-last-separator = false\n\n[tikv-importer]\n backend = &quot;local&quot;\n sorted-kv-dir = &quot;/kvdir/&quot;\n\ndisk-quota = &quot;1.5TiB&quot;\n\n[mydumper]\ndata-source-dir = &quot;/csv_dir/&quot;\nstrict-format = false\nno-schema = true\n\n[tidb]\nhost = &quot;xxx&quot;\nport = 3306\nuser = &quot;github_events&quot;\npassword = &quot;******&quot;\n\n[lightning]\ncheck-requirements = false\nregion-concurrency = 32\nmeta-schema-name = &quot;gharchive_meta&quot;\n</code></pre><ol start=\"3\"><li>Load the data into the TiDB cluster. </li></ol><pre><code class=\"language-bash\">nohup tidb-lightning -config ./tidb-lightning.toml &gt; nohup.out\n</code></pre><ol start=\"4\"><li>Convert the unstructured <code>json file</code> provided by GH Archive into structured data. </li></ol><pre><code class=\"language-sql\">gharchive_dev&gt; desc github_events;\n+--------------------+--------------+------+-----+---------+-------+\n| Field              | Type         | Null | Key | Default | Extra |\n+--------------------+--------------+------+-----+---------+-------+\n| id                 | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |\n| type               | varchar(255) | YES  | MUL | &lt;null&gt;  |       |\n| created_at         | datetime     | YES  | MUL | &lt;null&gt;  |       |\n| repo_id            | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |\n| repo_name          | varchar(255) | YES  | MUL | &lt;null&gt;  |       |\n| actor_id           | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |\n| actor_login        | varchar(255) | YES  | MUL | &lt;null&gt;  |       |\n| actor_location     | varchar(255) | YES  |     | &lt;null&gt;  |       |\n| language           | varchar(255) | YES  | MUL | &lt;null&gt;  |       |\n| additions          | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |\n| deletions          | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |\n| action             | varchar(255) | YES  | MUL | &lt;null&gt;  |       |\n| number             | int(11)      | YES  |     | &lt;null&gt;  |       |\n| commit_id          | varchar(255) | YES  | MUL | &lt;null&gt;  |       |\n| comment_id         | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |\n| org_login          | varchar(255) | YES  | MUL | &lt;null&gt;  |       |\n| org_id             | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |\n| state              | varchar(255) | YES  |     | &lt;null&gt;  |       |\n| closed_at          | datetime     | YES  | MUL | &lt;null&gt;  |       |\n| comments           | int(11)      | YES  | MUL | &lt;null&gt;  |       |\n| pr_merged_at       | datetime     | YES  | MUL | &lt;null&gt;  |       |\n| pr_merged          | tinyint(1)   | YES  |     | &lt;null&gt;  |       |\n| pr_changed_files   | int(11)      | YES  | MUL | &lt;null&gt;  |       |\n| pr_review_comments | int(11)      | YES  | MUL | &lt;null&gt;  |       |\n| pr_or_issue_id     | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |\n| event_day          | date         | YES  | MUL | &lt;null&gt;  |       |\n| event_month        | date         | YES  | MUL | &lt;null&gt;  |       |\n| author_association | varchar(255) | YES  |     | &lt;null&gt;  |       |\n| event_year         | int(11)      | YES  | MUL | &lt;null&gt;  |       |\n| push_size          | int(11)      | YES  |     | &lt;null&gt;  |       |\n| push_distinct_size | int(11)      | YES  |     | &lt;null&gt;  |       |\n+--------------------+--------------+------+-----+---------+-------+\n</code></pre><ol start=\"5\"><li>With structured data at hand, we can start to make further analysis with TiDB Cloud. Execute SQL commands to generate analytical results. For example, you can execute SQL commands below to output the top 10 most starred JavaScript framework repos in 2021.</li></ol><pre><code class=\"language-sql\">  SELECT js.name, count(*) as stars \n    FROM github_events \n         JOIN js_framework_repos js ON js.id = github_events.repo_id \n   WHERE type = &#x27;WatchEvent&#x27; and event_year = 2021 \nGROUP BY 1 \nORDER BY 2 DESC\n   LIMIT 10;\n+-------------------+-------+\n| name              | stars |\n+-------------------+-------+\n| facebook/react    | 22830 |\n| sveltejs/svelte   | 18573 |\n| vuejs/vue         | 18015 |\n| angular/angular   | 11037 |\n| alpinejs/alpine   | 6993  |\n| preactjs/preact   | 2965  |\n| hotwired/stimulus | 1355  |\n| marko-js/marko    | 1006  |\n| neomjs/neo        | 826   |\n| tastejs/todomvc   | 813   |\n+-------------------+-------+\n</code></pre><p>We have analyzed all the GitHub projects regarding databases, JavaScripe frameworks, programming languages, web frameworks, and low-code development tools, and provided valuable insights in 2021, in real time, and custom insights. If the repository you care about is not included here, you&#x27;re welcome to submit your PR <a href=\"https://github.com/hooopo/gharchive/tree/main/meta/repos\">here</a>. If you want to gain more insights into other areas, you can try TiDB Cloud by yourselves with this <a href=\"https://ossinsight.io/blog/try-it-yourself/\">10 minute tutorial</a>. </p><p>Below are the areas of GitHub projects we have analyzed. </p><pre><code class=\"language-sql\">gharchive_dev&gt; show tables;\n+-----------------------------+\n| Tables_in_gharchive_dev     |\n+-----------------------------+\n| cn_repos                    |\n| css_framework_repos         |\n| db_repos                    |\n| github_events               |\n| js_framework_repos          |\n| nocode_repos                |\n| programming_language_repos  |\n| static_site_generator_repos |\n| web_framework_repos         |\n+-----------------------------+\n</code></pre>",
            "url": "https://ossinsight.io/blog/how-it-works",
            "title": "Data Preparation for Analytics",
            "summary": "All the data we use here on this website sources from GH Archive, a non-profit project that records and archives all GitHub events data since 2011. The total data volume archived by GH Archive can be up to 4 billion rows. We download the json file on GH Archive and convert it into csv format via Script, and finally load it into the TiDB cluster in parallel through TiDB-Lightning.",
            "date_modified": "2022-04-19T04:45:13.663Z"
        },
        {
            "id": "/try-it-yourself",
            "content_html": "<p><a href=\"https://docs.pingcap.com/tidb/stable/overview\">TiDB</a> is an open source distributed NewSQL database with horizontal scalability, high availability, and strong consistency. It can also deal with mixed OLTP and OLAP workloads at the same time by leveraging its hybrid transactional and analytical (HTAP) capability. </p><p><strong><a href=\"https://docs.pingcap.com/tidbcloud/public-preview\">TiDB Cloud</a> is a fully-managed Database-as-a-Service (DBaaS)</strong> that brings everything great about TiDB to your cloud and lets you focus on your applications, not the complexities of your database. </p><p>In this tutorial, we will provide you with a piece of sample data of all GitHub events occurring on January 1, 2022, and walk you through on how to use TiDB Cloud to analyze this data in 10 minutes.  </p><h2>Sign up for a TiDB Cloud account (Free)</h2><ol><li>Click <a href=\"https://tidbcloud.com/signup\">here</a> to sign up for a TiDB Cloud account free of charge. </li><li><a href=\"https://tidbcloud.com/?__hstc=86493575.35e9113e934f84907657eac98c81808d.1645076526216.1646808171218.1646883397864.20&amp;__hssc=86493575.6.1646883397864&amp;__hsfp=2077185778\">Log in</a> to your account.</li></ol><h2>Create a TiDB Developer Tier cluster (Free)</h2><p>Once you register an account, you can create a free cluster with TiDB Developer Tier. </p><p>:::info\nA cluster is a database to store data.\n:::</p><ol><li>Click <strong>Get Started for Free</strong> and start to create a free cluster. </li></ol><p><img src=\"/img/try-it-yourself/dev-tier.png\"/></p><ol start=\"2\"><li>On the <strong>Create a Cluster</strong> page, set up your cluster name and root password.</li><li>Note that the cloud provider is AWS by default, and then select the region which is close to you to create the cluster.</li><li>The cluster tier is S1.dev by default.</li><li>Click <strong>Submit</strong>.\nYour TiDB Cloud cluster will be created in approximately 5 to 10 minutes.</li></ol><p>:::note\nThe Developer Tier is <strong>free</strong> for 1 year.\n:::</p><h2>Import data to your TiDB Cloud cluster</h2><h3>Import the data</h3><p>Once your cluster is ready, you can start to import the sample data to your cluster. </p><p>:::info\nWe have merged the create database/table in the SQL files, so you don&#x27;t need to <code>create database/tables</code> by yourself.</p><p>If you want to know the table schema, you can check <code>desc gharchive_dev</code> later in the following step.\n:::</p><ol><li>Click the <strong>Import</strong> button on the <strong>Active Clusters</strong> page and then go to the <strong>Data Import Task</strong> page. </li></ol><p><img src=\"/img/try-it-yourself/import.png\"/></p><ol start=\"2\"><li>Copy the values below and paste to the blanks of <strong>Bucket URL</strong> and <strong>Role-ARN</strong> respectively on the <strong>Data Import Task</strong> page.</li></ol><p><strong>Bucket URL</strong>:</p><pre><code>s3://tidbcloud-samples/gharchive/\n</code></pre><p><strong>Role-ARN</strong>:</p><pre><code>arn:aws:iam::385595570414:role/import-sample-access\n</code></pre><ol start=\"3\"><li>Choose <strong>US West (Oregon)</strong> for your <strong>Bucket region</strong>;</li><li>Tick <strong>TiDB Dumpling</strong> for the <strong>Data Format</strong>. </li><li>Input your cluster password in the blank of Password on the <strong>Target Database</strong> section. </li></ol><p><img src=\"/img/try-it-yourself/fill.png\"/></p><ol start=\"6\"><li>After you fill in all the blanks on the <strong>Data Import Task</strong> page, click the <strong>Import</strong> button at the bottom of this page and wait for a few moments for the system to complete data importing. </li></ol><h3>Use the web shell to check if data is ready</h3><p>TiDB Cloud provides a web shell to connect the database online. </p><ol><li>Click the <strong>Exit</strong> button after you successfully import the data into your cluster. </li><li>Then, click the <strong>Connect</strong> button and the <strong>Connect to TiDB</strong> panel pops out. </li><li>Choose <strong>Web SQL Shell</strong> --&gt; <strong>Open SQL Shell</strong>. </li><li>Then input your cluster password as shown in the image below.</li></ol><p><img src=\"/img/try-it-yourself/web-shell.png\"/></p><h3>Set column storage replica: TiFlash (Optional)</h3><p><a href=\"https://docs.pingcap.com/tidb/stable/tiflash-overview\">TiFlash</a> is the key component that makes TiDB / TiDB Cloud an HTAP database and capable of dealing with OLTP and OLAP workloads at the same time. </p><p>Here, you can try the following SQL commands on TiDB Cloud to experience its real-time analytics with ease.</p><ol><li>Execute the SQL statements specified below </li></ol><pre><code class=\"language-sql\">use gharchive_dev;\nALTER TABLE github_events SET TIFLASH REPLICA 1;\nALTER TABLE db_repos SET TIFLASH REPLICA 1;\n</code></pre><ol start=\"2\"><li>Setting a TiFlash replica will take you some time, so you can use the following SQL statements to check if the procedure is done or not. </li></ol><pre><code class=\"language-sql\">SELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = &#x27;gharchive_dev&#x27; and TABLE_NAME = &#x27;github_events&#x27;;\n</code></pre><p>If the results you get are the same as follows, then it means the procedure is done. </p><pre><code class=\"language-sql\">mysql&gt; SELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = &#x27;gharchive_dev&#x27; and TABLE_NAME = &#x27;github_events&#x27;;\n+---------------+---------------+----------+---------------+-----------------+-----------+----------+\n| TABLE_SCHEMA  | TABLE_NAME    | TABLE_ID | REPLICA_COUNT | LOCATION_LABELS | AVAILABLE | PROGRESS |\n+---------------+---------------+----------+---------------+-----------------+-----------+----------+\n| gharchive_dev | github_events |       68 |             1 |                 |         1 |        1 |\n+---------------+---------------+----------+---------------+-----------------+-----------+----------+\n1 row in set (0.27 sec)\n\nmysql&gt;\n</code></pre><h2>Analysis!</h2><p>After you finish all the steps above, you can start the analytical process. </p><p>:::tip\nIf you want to know the table schema, you can use <code>show create table tbl_name</code> to get that information.\n:::</p><p>Because you have imported the sample data of all GitHub events occurred on the first hour of 2022 (from 2022-01-01 00:00:00 to 2022-01-01 00:59:59), you can start to make any queries based on that data by using SQL commands. </p><h3>How many events occurred in total?</h3><p>Execute the following SQL statement to query the total number of events. </p><pre><code class=\"language-sql\">SELECT count(*) FROM github_events\n</code></pre><h3>Which repository gets the most stars?</h3><p>Execute the following statements to query the most starred repository. </p><pre><code class=\"language-sql\">  SELECT repo_name, count(*) AS events_count\n    FROM github_events\n   WHERE type = &#x27;WatchEvent&#x27; /* Yes, `WatchEvent` means star */\nGROUP BY 1\nORDER BY 2 DESC\n   LIMIT 20;\n</code></pre><h2>Mini Test</h2><p>Here is a small test for you to practice how to use TiDB Cloud to conduct analytics. </p><h3>Q: Who is the most active contributor except the robot accounts on the first hour of 2022?</h3><h3>Click for the answer. ⬇️</h3><details><summary>Click me to show answer</summary><p><pre><code class=\"language-sql\">  SELECT actor_login, \n         count(*) AS events_count\n    FROM github_events\n   WHERE actor_login NOT LIKE &#x27;%bot%&#x27;\nGROUP BY 1\nORDER BY 2 DESC \n   LIMIT 20\n</code></pre></p></details><h2>Watch the video below for detailed information</h2><video width=\"100%\" poster=\"/img/try-it-yourself/dev-tier.png\" controls=\"\"><source src=\"/video/github-demo-tidbcloud.mp4\" type=\"video/mp4\"/></video>",
            "url": "https://ossinsight.io/blog/try-it-yourself",
            "title": "▶️  Use TiDB Cloud to Analyze GitHub Events in 10 Minutes",
            "summary": "TiDB is an open source distributed NewSQL database with horizontal scalability, high availability, and strong consistency. It can also deal with mixed OLTP and OLAP workloads at the same time by leveraging its hybrid transactional and analytical (HTAP) capability.",
            "date_modified": "2022-04-19T04:45:13.663Z"
        },
        {
            "id": "/try-your-own-dataset",
            "url": "https://ossinsight.io/blog/try-your-own-dataset",
            "title": "Try Your Own Dataset",
            "summary": "🔥 Use TiDB Cloud to Analyze GitHub Events in 10 Minutes",
            "date_modified": "2022-04-19T04:45:13.663Z"
        }
    ]
}