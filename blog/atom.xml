<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://ossinsight.io/blog</id>
    <title>Open Source Software Insight Blog</title>
    <updated>2022-04-24T08:27:01.030Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://ossinsight.io/blog"/>
    <subtitle>Open Source Software Insight Blog</subtitle>
    <icon>https://ossinsight.io/img/favicon.png</icon>
    <entry>
        <title type="html"><![CDATA[Build a Real-time Analytics Dashboard in Logistics Industry with Metabase]]></title>
        <id>/build-a-real-time-analytics-dashboard-in-metabase</id>
        <link href="https://ossinsight.io/blog/build-a-real-time-analytics-dashboard-in-metabase"/>
        <updated>2022-04-24T08:27:01.030Z</updated>
        <summary type="html"><![CDATA[#Real-time analytics]]></summary>
        <content type="html"><![CDATA[<p><code>#Real-time analytics</code>
<!-- --> <!-- -->
<code>#Proliferate data</code>
<!-- --> <!-- -->
<code>#Logistics industry</code></p><p>In this tutorial, you will build a prototype for PingExpress_DemoCorp’s real-time analytics dashboard that runs on a TiDB Cloud Proof-of-Concept (PoC) cluster.</p><p><img src="https://en.pingcap.com/wp-content/uploads/2022/03/Metabase-dashboard-overview.png"/></p><blockquote><p>Disclaimer:</p><ul><li>PingExpress_DemoCorp is a dummy company. It does NOT reflect or imply any real company.</li><li>This tutorial is for demonstration purposes only. Do NOT use any material (including but not limited to code, and commands) from this tutorial in production environments.</li></ul></blockquote><p>PingExpress_DemoCorp is a supply chain management company in the United States. With more people shopping online due to the pandemic, it’s business has scaled rapidly. They’re delivering tens of billions of packages a year.</p><p>With so many deliveries, a key part of their success is accurate and efficient package tracking. Business managers need to know where packages are so they can identify potential traffic blocks and rearrange delivery routes. Customers need accurate delivery dates so they can plan ahead. Therefore, real-time tracking, status updates, and a detailed dashboard are very important to PingExpress_DemoCorp.</p><p><strong>With the current technology infra, PingExpress_DemoCorp is facing growing pains:</strong></p><p>PingExpress_DemoCorp uses MySQL. For a real-time dashboard, they need to use both historical data and new data coming in. Data analytics rely on stored procedures. As business roars, more data needs to be stored. The MySQL sharding solution can’t meet their requirements, and the system is hard to scale and maintain.
During peak hours, the performance on a standalone machine is poor. There is also high risk of a single-point failure.</p><p><strong>PingExpress_DemoCorp considered two options:</strong></p><blockquote><p><strong>Option A:</strong> Add a dedicated column store to the existing data stack to separate the OLTP workload from the OLAP workload.</p></blockquote><blockquote><p><strong>Option B:</strong> Replace MySQL database with TiDB, which contains both the row store for daily transactions and the column store for analytical workloads.</p></blockquote><p>PingExpress_DemoCorp chose <strong>option B</strong>. </p><p>This is because adding another column storage for analysis workload makes the system more complicated. At the same time, data has to be synchronized from the row store to the column store via painful ETL processes over night. This means that choosing option A still does not enable PingExpress_DemoCorp to do real-time analytics.</p><p><strong>On the other hand, switching to <a href="https://en.pingcap.com/tidb-cloud/?utm_source=ossinsight">TiDB Cloud</a>   as the backend database is very attractive to PingExpress_DemoCorp：</strong></p><p>In this tutorial, you will build a prototype for PingExpress_DemoCorp’s real-time analytics dashboard that runs on a
TiDB Cloud Proof-of-Concept (PoC) cluster.</p><h2>Before you begin</h2><p>You should have the following software and packages installed:</p><ul><li>Python (v. 3+)</li><li>MySQL connector for Python</li><li>SQLAlchemy</li><li>sqlalchemy-tidb</li><li>Metabase</li></ul><p>Note: It is recommended to use pip3 to install packages, such as SQLAlchemy. We also suggest NOT to use the Mac application version for Metabase. It is gradually being phased out. You may use the jar version instead.</p><p>:::info
<strong> We recomand you Start with <a href="https://docs.pingcap.com/tidbcloud/?utm_source=ossinsight">TiDB Cloud Documentation</a> and finish this <a href="https://ossinsight.io/blog/try-it-yourself/">10-minute tutorial</a> first.</strong>
:::</p><h2>1. Create a dashboard.</h2><ol><li>In the top right corner of the dashboard, click the <strong>+</strong> sign, and then choose <strong>New Dashboard</strong>.</li><li>Enter the name as <strong>PingExpress_dashboard</strong>.</li><li>Click <strong><em>Create</em></strong>.</li></ol><h2>2. Add a question.</h2><ol><li>In the top right corner, click <strong>Ask a question</strong> on the top right corner, and then choose <strong>Native query</strong>.</li><li>Select <strong>PingExpressDB</strong> as the database.</li><li>Display the total number of packages delivered. Enter the following query and click the right side of the screen to run it:
<code>SELECT COUNT(*) FROM packages WHERE transaction_kind=&quot;4_pkg_out&quot;;</code></li></ol><h2>3. Save the question.</h2><ol><li>In the upper right corner, click <strong>Save</strong>.</li><li>Enter the name <strong>Total packages delivered</strong>.</li><li>When being asked if you would like to add this question to the dashboard, click <strong>Yes please!</strong>, and choose <strong>PingExpress_dashboard</strong>.
The result will now appear on the dashboard.</li><li>Click <strong>Save</strong>.</li></ol><h2>4. Repeat steps 2 and 3 for the second question, “Number of packages on the way.” This is the query to use:</h2><p><code>SELECT COUNT(*) FROM packages WHERE transaction_kind != &quot;4_pkg_out&quot;;</code></p><h2>5. Visualize the <strong>Number of packages in process in each state</strong>.</h2><ol><li>Repeat step 3 and use the following query instead.
<code>SELECT start_state, COUNT(package_id)</code></li><li>After getting the result, click the <strong>Visualization</strong> button, and then choose <strong>Map</strong>. For the map options:</li></ol><ul><li>Map Type: Region map</li><li>Region Map: United States</li><li>Leave everything else as default.</li></ul><ol start="3"><li>Repeat step 4 and add this question to the dashboard.
<img src="https://en.pingcap.com/wp-content/uploads/2022/03/Add-questions-to-dashboard-768x602.png"/></li></ol><h2>6. (Optional) Repeat the previous steps to add two more queries:</h2><ol><li><p>Number of packages in each stage (pie chart):
<code>SELECT transaction_kind, count(*) </code></p></li><li><p>Number of new packages per day (line chart):
<code>SELECT DATE(start_time), count(*) </code></p></li></ol>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Changelog]]></title>
        <id>/changelog</id>
        <link href="https://ossinsight.io/blog/changelog"/>
        <updated>2022-04-24T08:27:01.030Z</updated>
        <summary type="html"><![CDATA[2022-03-22]]></summary>
        <content type="html"><![CDATA[<h2>2022-03-22</h2><ul><li>Add &quot;Compare Projects&quot; tools</li><li>Add &quot;Try It Yourself&quot; blog</li></ul>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Difference Between MySQL Compatible Databases ...]]></title>
        <id>/difference-between-mysql-compatible-databases</id>
        <link href="https://ossinsight.io/blog/difference-between-mysql-compatible-databases"/>
        <updated>2022-04-24T08:27:01.030Z</updated>
        <summary type="html"><![CDATA[Contributors]]></summary>
        <content type="html"><![CDATA[<h2>Contributors</h2><blockquote><p>Contributors open pull requests, issues and comment in pr body &amp; issue body</p></blockquote><iframe width="100%" height="400" src="/charts/tidb-vs-mysql-compatible-databases-contributor.html?theme=vintage&amp;v=3"></iframe><h2>Contributions</h2><blockquote><p>Total Number of Pull Request + Issue + Comment + Review</p></blockquote><iframe width="100%" height="400" src="/charts/tidb-vs-mysql-compatible-databases-contribution.html?theme=vintage&amp;v=3"></iframe><h2>Code</h2><blockquote><p>lines of modified code: additions + deletions</p></blockquote><iframe width="100%" height="400" src="/charts/tidb-vs-mysql-compatible-databases-code.html?theme=vintage&amp;v=3"></iframe><h3>The top 10 pull request code additions+deletions of <code>percona/percona-server</code></h3><pre><code class="language-sql">gharchive_dev&gt; select (additions+deletions) as lines_modified, concat(&#x27;https://github.com/percona/percona-server/pull/&#x27;, number) from github_ev
            -&gt; ents where repo_name = &#x27;percona/percona-server&#x27; order by lines_modified desc limit 10;
+----------------+-------------------------------------------------------------------+
| lines_modified | concat(&#x27;https://github.com/percona/percona-server/pull/&#x27;, number) |
+----------------+-------------------------------------------------------------------+
| 1847591        | https://github.com/percona/percona-server/pull/3474               |
| 1847131        | https://github.com/percona/percona-server/pull/3474               |
| 1611523        | https://github.com/percona/percona-server/pull/3978               |
| 1611239        | https://github.com/percona/percona-server/pull/3978               |
| 1526190        | https://github.com/percona/percona-server/pull/4204               |
| 1525900        | https://github.com/percona/percona-server/pull/4235               |
| 1525495        | https://github.com/percona/percona-server/pull/4235               |
| 1436855        | https://github.com/percona/percona-server/pull/4204               |
| 919569         | https://github.com/percona/percona-server/pull/4407               |
| 831538         | https://github.com/percona/percona-server/pull/3604               |
+----------------+-------------------------------------------------------------------+
10 rows in set
Time: 0.168s
gharchive_dev&gt;
</code></pre><h2>Pull Requests</h2><blockquote><p>Pull requests trend</p></blockquote><iframe width="100%" height="400" src="/charts/tidb-vs-mysql-compatible-databases-pull-request.html?theme=vintage&amp;v=3"></iframe>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Preparation for Analytics]]></title>
        <id>/how-it-works</id>
        <link href="https://ossinsight.io/blog/how-it-works"/>
        <updated>2022-04-24T08:27:01.030Z</updated>
        <summary type="html"><![CDATA[All the data we use here on this website sources from GH Archive, a non-profit project that records and archives all GitHub events data since 2011. The total data volume archived by GH Archive can be up to 4 billion rows. We download the json file on GH Archive and convert it into csv format via Script, and finally load it into the TiDB cluster in parallel through TiDB-Lightning.]]></summary>
        <content type="html"><![CDATA[<p>All the data we use here on this website sources from <a href="https://www.gharchive.org/">GH Archive</a>, a non-profit project that records and archives all GitHub events data since 2011. The total data volume archived by GH Archive can be up to 4 billion rows. We download the <code>json file</code> on GH Archive and convert it into csv format via Script, and finally load it into the TiDB cluster in parallel through <a href="https://docs.pingcap.com/tidb/stable/tidb-lightning-overview">TiDB-Lightning</a>.</p><p>In this post, we will explain step by step how we conduct this process. </p><ol><li>Prepare the data in csv format for TiDB Lighting. </li></ol><pre><code>├── gharchive_dev.github_events.000000000000.csv
├── gharchive_dev.github_events.000000000001.csv
├── gharchive_dev.github_events.000000000002.csv
├── gharchive_dev.github_events.000000000003.csv
├── gharchive_dev.github_events.000000000004.csv
├── gharchive_dev.github_events.000000000005.csv
├── gharchive_dev.github_events.000000000006.csv
├── gharchive_dev.github_events.000000000007.csv
├── gharchive_dev.github_events.000000000008.csv
├── gharchive_dev.github_events.000000000009.csv
├── gharchive_dev.github_events.000000000010.csv
├── gharchive_dev.github_events.000000000011.csv
├── gharchive_dev.github_events.000000000012.csv
├── gharchive_dev.github_events.000000000013.csv
</code></pre><ol start="2"><li>Configure the TiDB Lightning as follows.</li></ol><pre><code>cat tidb-lightning.toml
[mydumper.csv]
separator = &#x27;,&#x27;
delimiter = &#x27;&quot;&#x27;
header = true
not-null = false
backslash-escape = true
trim-last-separator = false

[tikv-importer]
 backend = &quot;local&quot;
 sorted-kv-dir = &quot;/kvdir/&quot;

disk-quota = &quot;1.5TiB&quot;

[mydumper]
data-source-dir = &quot;/csv_dir/&quot;
strict-format = false
no-schema = true

[tidb]
host = &quot;xxx&quot;
port = 3306
user = &quot;github_events&quot;
password = &quot;******&quot;

[lightning]
check-requirements = false
region-concurrency = 32
meta-schema-name = &quot;gharchive_meta&quot;
</code></pre><ol start="3"><li>Load the data into the TiDB cluster. </li></ol><pre><code class="language-bash">nohup tidb-lightning -config ./tidb-lightning.toml &gt; nohup.out
</code></pre><ol start="4"><li>Convert the unstructured <code>json file</code> provided by GH Archive into structured data. </li></ol><pre><code class="language-sql">gharchive_dev&gt; desc github_events;
+--------------------+--------------+------+-----+---------+-------+
| Field              | Type         | Null | Key | Default | Extra |
+--------------------+--------------+------+-----+---------+-------+
| id                 | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| type               | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| created_at         | datetime     | YES  | MUL | &lt;null&gt;  |       |
| repo_id            | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| repo_name          | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| actor_id           | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| actor_login        | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| actor_location     | varchar(255) | YES  |     | &lt;null&gt;  |       |
| language           | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| additions          | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| deletions          | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| action             | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| number             | int(11)      | YES  |     | &lt;null&gt;  |       |
| commit_id          | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| comment_id         | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| org_login          | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| org_id             | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| state              | varchar(255) | YES  |     | &lt;null&gt;  |       |
| closed_at          | datetime     | YES  | MUL | &lt;null&gt;  |       |
| comments           | int(11)      | YES  | MUL | &lt;null&gt;  |       |
| pr_merged_at       | datetime     | YES  | MUL | &lt;null&gt;  |       |
| pr_merged          | tinyint(1)   | YES  |     | &lt;null&gt;  |       |
| pr_changed_files   | int(11)      | YES  | MUL | &lt;null&gt;  |       |
| pr_review_comments | int(11)      | YES  | MUL | &lt;null&gt;  |       |
| pr_or_issue_id     | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| event_day          | date         | YES  | MUL | &lt;null&gt;  |       |
| event_month        | date         | YES  | MUL | &lt;null&gt;  |       |
| author_association | varchar(255) | YES  |     | &lt;null&gt;  |       |
| event_year         | int(11)      | YES  | MUL | &lt;null&gt;  |       |
| push_size          | int(11)      | YES  |     | &lt;null&gt;  |       |
| push_distinct_size | int(11)      | YES  |     | &lt;null&gt;  |       |
+--------------------+--------------+------+-----+---------+-------+
</code></pre><ol start="5"><li>With structured data at hand, we can start to make further analysis with TiDB Cloud. Execute SQL commands to generate analytical results. For example, you can execute SQL commands below to output the top 10 most starred JavaScript framework repos in 2021.</li></ol><pre><code class="language-sql">  SELECT js.name, count(*) as stars 
    FROM github_events 
         JOIN js_framework_repos js ON js.id = github_events.repo_id 
   WHERE type = &#x27;WatchEvent&#x27; and event_year = 2021 
GROUP BY 1 
ORDER BY 2 DESC
   LIMIT 10;
+-------------------+-------+
| name              | stars |
+-------------------+-------+
| facebook/react    | 22830 |
| sveltejs/svelte   | 18573 |
| vuejs/vue         | 18015 |
| angular/angular   | 11037 |
| alpinejs/alpine   | 6993  |
| preactjs/preact   | 2965  |
| hotwired/stimulus | 1355  |
| marko-js/marko    | 1006  |
| neomjs/neo        | 826   |
| tastejs/todomvc   | 813   |
+-------------------+-------+
</code></pre><p>We have analyzed all the GitHub projects regarding databases, JavaScripe frameworks, programming languages, web frameworks, and low-code development tools, and provided valuable insights in 2021, in real time, and custom insights. If the repository you care about is not included here, you&#x27;re welcome to submit your PR <a href="https://github.com/hooopo/gharchive/tree/main/meta/repos">here</a>. If you want to gain more insights into other areas, you can try TiDB Cloud by yourselves with this <a href="https://ossinsight.io/blog/try-it-yourself/">10 minute tutorial</a>. </p><p>Below are the areas of GitHub projects we have analyzed. </p><pre><code class="language-sql">gharchive_dev&gt; show tables;
+-----------------------------+
| Tables_in_gharchive_dev     |
+-----------------------------+
| cn_repos                    |
| css_framework_repos         |
| db_repos                    |
| github_events               |
| js_framework_repos          |
| nocode_repos                |
| programming_language_repos  |
| static_site_generator_repos |
| web_framework_repos         |
+-----------------------------+
</code></pre>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[▶️  Use TiDB Cloud to Analyze GitHub Events in 10 Minutes]]></title>
        <id>/try-it-yourself</id>
        <link href="https://ossinsight.io/blog/try-it-yourself"/>
        <updated>2022-04-24T08:27:01.030Z</updated>
        <summary type="html"><![CDATA[TiDB is an open source distributed NewSQL database with horizontal scalability, high availability, and strong consistency. It can also deal with mixed OLTP and OLAP workloads at the same time by leveraging its hybrid transactional and analytical (HTAP) capability.]]></summary>
        <content type="html"><![CDATA[<p><a href="https://docs.pingcap.com/tidb/stable/overview?utm_source=ossinsight">TiDB</a> is an open source distributed NewSQL database with horizontal scalability, high availability, and strong consistency. It can also deal with mixed OLTP and OLAP workloads at the same time by leveraging its hybrid transactional and analytical (HTAP) capability. </p><p><strong><a href="https://docs.pingcap.com/tidbcloud/public-preview?utm_source=ossinsight">TiDB Cloud</a> is a fully-managed Database-as-a-Service (DBaaS)</strong> that brings everything great about TiDB to your cloud and lets you focus on your applications, not the complexities of your database. </p><p>In this tutorial, we will provide you with a piece of sample data of all GitHub events occurring on January 1, 2022, and walk you through on how to use TiDB Cloud to analyze this data in 10 minutes.  </p><h2>Sign up for a TiDB Cloud account (Free)</h2><ol><li>Click <a href="https://tidbcloud.com/signup?utm_source=ossinsight">here</a> to sign up for a TiDB Cloud account free of charge. </li><li><a href="https://tidbcloud.com/?utm_source=ossinsight">Log in</a> to your account.</li></ol><h2>Create a TiDB Developer Tier cluster (Free)</h2><p>Once you register an account, you can create a free cluster with TiDB Developer Tier. </p><p>:::info
A cluster is a database to store data.
:::</p><ol><li>Click <strong>Get Started for Free</strong> and start to create a free cluster. </li></ol><p><img src="/img/try-it-yourself/dev-tier.png"/></p><ol start="2"><li>On the <strong>Create a Cluster</strong> page, set up your cluster name and root password.</li><li>Note that the cloud provider is AWS by default, and then select the <code>US-West-2 (Oregon)</code> region to create the cluster.</li><li>The cluster tier is S1.dev by default.</li><li>Click <strong>Submit</strong>.
Your TiDB Cloud cluster will be created in approximately 5 to 10 minutes.</li></ol><p>:::note
The Developer Tier is <strong>free</strong> for 1 year.
:::</p><h2>Import data to your TiDB Cloud cluster</h2><h3>Import the data</h3><p>Once your cluster is ready, you can start to import the sample data to your cluster. </p><p>:::info
We have merged the create database/table in the SQL files, so you don&#x27;t need to <code>create database/tables</code> by yourself.</p><p>If you want to know the table schema, you can check <code>desc gharchive_dev</code> later in the following step.
:::</p><ol><li>Click the <strong>Import</strong> button on the <strong>Active Clusters</strong> page and then go to the <strong>Data Import Task</strong> page. </li></ol><p><img src="/img/try-it-yourself/import.png"/></p><ol start="2"><li>Copy the values below and paste to the blanks of <strong>Bucket URL</strong> and <strong>Role-ARN</strong> respectively on the <strong>Data Import Task</strong> page.</li></ol><p><strong>Bucket URL</strong>:</p><pre><code>s3://tidbcloud-samples/gharchive/
</code></pre><p><strong>Role-ARN</strong>:</p><pre><code>arn:aws:iam::385595570414:role/import-sample-access
</code></pre><ol start="3"><li>Choose <strong>US West (Oregon)</strong> for your <strong>Bucket region</strong>;</li><li>Tick <strong>TiDB Dumpling</strong> for the <strong>Data Format</strong>. </li><li>Input your cluster password in the blank of Password on the <strong>Target Database</strong> section. </li></ol><p><img src="/img/try-it-yourself/fill.png"/></p><ol start="6"><li>After you fill in all the blanks on the <strong>Data Import Task</strong> page, click the <strong>Import</strong> button at the bottom of this page and wait for a few moments for the system to complete data importing. </li></ol><h3>Use the web shell to check if data is ready</h3><p>TiDB Cloud provides a web shell to connect the database online. </p><ol><li>Click the <strong>Exit</strong> button after you successfully import the data into your cluster. </li><li>Then, click the <strong>Connect</strong> button and the <strong>Connect to TiDB</strong> panel pops out. </li><li>Choose <strong>Web SQL Shell</strong> --&gt; <strong>Open SQL Shell</strong>. </li><li>Then input your cluster password as shown in the image below.</li></ol><p><img src="/img/try-it-yourself/web-shell.png"/></p><h3>Set column storage replica: TiFlash (Optional)</h3><p><a href="https://docs.pingcap.com/tidb/stable/tiflash-overview?utm_source=ossinsight">TiFlash</a> is the key component that makes TiDB / TiDB Cloud an HTAP database and capable of dealing with OLTP and OLAP workloads at the same time. </p><p>Here, you can try the following SQL commands on TiDB Cloud to experience its real-time analytics with ease.</p><ol><li>Execute the SQL statements specified below </li></ol><pre><code class="language-sql">use gharchive_dev;
ALTER TABLE github_events SET TIFLASH REPLICA 1;
</code></pre><ol start="2"><li>Setting a TiFlash replica will take you some time, so you can use the following SQL statements to check if the procedure is done or not. </li></ol><pre><code class="language-sql">SELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = &#x27;gharchive_dev&#x27; and TABLE_NAME = &#x27;github_events&#x27;;
</code></pre><p>If the results you get are the same as follows, then it means the procedure is done. </p><pre><code class="language-sql">mysql&gt; SELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = &#x27;gharchive_dev&#x27; and TABLE_NAME = &#x27;github_events&#x27;;
+---------------+---------------+----------+---------------+-----------------+-----------+----------+
| TABLE_SCHEMA  | TABLE_NAME    | TABLE_ID | REPLICA_COUNT | LOCATION_LABELS | AVAILABLE | PROGRESS |
+---------------+---------------+----------+---------------+-----------------+-----------+----------+
| gharchive_dev | github_events |       68 |             1 |                 |         1 |        1 |
+---------------+---------------+----------+---------------+-----------------+-----------+----------+
1 row in set (0.27 sec)

mysql&gt;
</code></pre><h2>Analysis!</h2><p>After you finish all the steps above, you can start the analytical process. </p><p>:::tip
If you want to know the table schema, you can use <code>show create table tbl_name</code> to get that information.
:::</p><p>Because you have imported the sample data of all GitHub events occurred on the first hour of 2022 (from 2022-01-01 00:00:00 to 2022-01-01 00:59:59), you can start to make any queries based on that data by using SQL commands. </p><h3>How many events occurred in total?</h3><p>Execute the following SQL statement to query the total number of events. </p><pre><code class="language-sql">SELECT count(*) FROM github_events;
</code></pre><h3>Which repository gets the most stars?</h3><p>Execute the following statements to query the most starred repository. </p><pre><code class="language-sql">  SELECT repo_name, count(*) AS events_count
    FROM github_events
   WHERE type = &#x27;WatchEvent&#x27; /* Yes, `WatchEvent` means star */
GROUP BY 1
ORDER BY 2 DESC
   LIMIT 20;
</code></pre><h2>Mini Test</h2><p>Here is a small test for you to practice how to use TiDB Cloud to conduct analytics. </p><h3>Q: Who is the most active contributor except the robot accounts on the first hour of 2022?</h3><h3>Click for the answer. ⬇️</h3><details><summary>Click me to show answer</summary><p><pre><code class="language-sql">  SELECT actor_login, 
         count(*) AS events_count
    FROM github_events
   WHERE actor_login NOT LIKE &#x27;%bot%&#x27;
GROUP BY 1
ORDER BY 2 DESC 
   LIMIT 20
</code></pre></p></details><h2>Watch the video below for detailed information</h2><video width="100%" poster="/img/try-it-yourself/dev-tier.png" controls=""><source src="/video/github-demo-tidbcloud.mp4" type="video/mp4"/></video>]]></content>
    </entry>
</feed>