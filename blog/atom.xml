<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://ossinsight.io/blog</id>
    <title>Open Source Software Insight Blog</title>
    <updated>2022-04-22T02:51:36.668Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://ossinsight.io/blog"/>
    <subtitle>Open Source Software Insight Blog</subtitle>
    <icon>https://ossinsight.io/img/favicon.png</icon>
    <entry>
        <title type="html"><![CDATA[About US]]></title>
        <id>/about</id>
        <link href="https://ossinsight.io/blog/about"/>
        <updated>2022-04-22T02:51:36.668Z</updated>
        <summary type="html"><![CDATA[Who we are]]></summary>
        <content type="html"><![CDATA[<h3>Who we are</h3><p>PingCAP started in 2015 when three seasoned infrastructure engineers were sick and tired of the way databases were managed, scaled, and maintained while working at leading Internet companies.</p><p>Seeing no good solution on the market, they decided to build one themselves — the open-source way.</p><p>With the help of a first-class team, and hundreds of contributors from around the globe, PingCAP is building an open-source distributed NewSQL Hybrid Transactional and Analytical Processing (HTAP) database. TiDB, our flagship project, is a cloud-native distributed SQL layer with MySQL compatibility, and one of the most popular open-source database projects in the world (don’t take our word for it, check it out). TiDB’s sister project, TiKV, is a cloud-native distributed Key-Value store. It is now a CNCF Graduated project.</p><p><a href="https://pingcap.com/about-us/">Learn more about PingCAP</a></p>]]></content>
        <author>
            <name>PingCAP</name>
            <uri>https://github.com/pingcap</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Changelog]]></title>
        <id>/changelog</id>
        <link href="https://ossinsight.io/blog/changelog"/>
        <updated>2022-04-22T02:51:36.668Z</updated>
        <summary type="html"><![CDATA[2022-03-22]]></summary>
        <content type="html"><![CDATA[<h2>2022-03-22</h2><ul><li>Add &quot;Compare Projects&quot; tools</li><li>Add &quot;Try It Yourself&quot; blog</li></ul>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Difference Between MySQL Compatible Databaases ...]]></title>
        <id>/difference-between-mysql-compatible-databases</id>
        <link href="https://ossinsight.io/blog/difference-between-mysql-compatible-databases"/>
        <updated>2022-04-22T02:51:36.668Z</updated>
        <summary type="html"><![CDATA[Contributors]]></summary>
        <content type="html"><![CDATA[<h2>Contributors</h2><blockquote><p>Contributors open pull requests, issues and comment in pr body &amp; issue body</p></blockquote><iframe width="90%" height="400" src="/charts/tidb-vs-mysql-compatible-databases-contributor.html?theme=vintage"></iframe><h2>Contributions</h2><blockquote><p>Total Number of Pull Request + Issue + Comment + Review</p></blockquote><iframe width="90%" height="400" src="/charts/tidb-vs-mysql-compatible-databases-contribution.html?theme=vintage"></iframe><h2>Code</h2><blockquote><p>lines of modified code: additions + deletions</p></blockquote><iframe width="90%" height="400" src="/charts/tidb-vs-mysql-compatible-databases-code.html?theme=vintage&amp;v=2"></iframe><h3>The top 10 pull request code additions+deletions of <code>percona/percona-server</code></h3><pre><code class="language-sql">gharchive_dev&gt; select (additions+deletions) as lines_modified, concat(&#x27;https://github.com/percona/percona-server/pull/&#x27;, number) from github_ev
            -&gt; ents where repo_name = &#x27;percona/percona-server&#x27; order by lines_modified desc limit 10;
+----------------+-------------------------------------------------------------------+
| lines_modified | concat(&#x27;https://github.com/percona/percona-server/pull/&#x27;, number) |
+----------------+-------------------------------------------------------------------+
| 1847591        | https://github.com/percona/percona-server/pull/3474               |
| 1847131        | https://github.com/percona/percona-server/pull/3474               |
| 1611523        | https://github.com/percona/percona-server/pull/3978               |
| 1611239        | https://github.com/percona/percona-server/pull/3978               |
| 1526190        | https://github.com/percona/percona-server/pull/4204               |
| 1525900        | https://github.com/percona/percona-server/pull/4235               |
| 1525495        | https://github.com/percona/percona-server/pull/4235               |
| 1436855        | https://github.com/percona/percona-server/pull/4204               |
| 919569         | https://github.com/percona/percona-server/pull/4407               |
| 831538         | https://github.com/percona/percona-server/pull/3604               |
+----------------+-------------------------------------------------------------------+
10 rows in set
Time: 0.168s
gharchive_dev&gt;
</code></pre><h2>Pull Requests</h2><blockquote><p>Pull requests trend</p></blockquote><iframe width="90%" height="400" src="/charts/tidb-vs-mysql-compatible-databases-pull-request.html?theme=vintage"></iframe>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Preparation for Analytics]]></title>
        <id>/how-it-works</id>
        <link href="https://ossinsight.io/blog/how-it-works"/>
        <updated>2022-04-22T02:51:36.668Z</updated>
        <summary type="html"><![CDATA[All the data we use here on this website sources from GH Archive, a non-profit project that records and archives all GitHub events data since 2011. The total data volume archived by GH Archive can be up to 4 billion rows. We download the json file on GH Archive and convert it into csv format via Script, and finally load it into the TiDB cluster in parallel through TiDB-Lightning.]]></summary>
        <content type="html"><![CDATA[<p>All the data we use here on this website sources from <a href="https://www.gharchive.org/">GH Archive</a>, a non-profit project that records and archives all GitHub events data since 2011. The total data volume archived by GH Archive can be up to 4 billion rows. We download the <code>json file</code> on GH Archive and convert it into csv format via Script, and finally load it into the TiDB cluster in parallel through <a href="https://docs.pingcap.com/tidb/stable/tidb-lightning-overview">TiDB-Lightning</a>.</p><p>In this post, we will explain step by step how we conduct this process. </p><ol><li>Prepare the data in csv format for TiDB Lighting. </li></ol><pre><code>├── gharchive_dev.github_events.000000000000.csv
├── gharchive_dev.github_events.000000000001.csv
├── gharchive_dev.github_events.000000000002.csv
├── gharchive_dev.github_events.000000000003.csv
├── gharchive_dev.github_events.000000000004.csv
├── gharchive_dev.github_events.000000000005.csv
├── gharchive_dev.github_events.000000000006.csv
├── gharchive_dev.github_events.000000000007.csv
├── gharchive_dev.github_events.000000000008.csv
├── gharchive_dev.github_events.000000000009.csv
├── gharchive_dev.github_events.000000000010.csv
├── gharchive_dev.github_events.000000000011.csv
├── gharchive_dev.github_events.000000000012.csv
├── gharchive_dev.github_events.000000000013.csv
</code></pre><ol start="2"><li>Configure the TiDB Lightning as follows.</li></ol><pre><code>cat tidb-lightning.toml
[mydumper.csv]
separator = &#x27;,&#x27;
delimiter = &#x27;&quot;&#x27;
header = true
not-null = false
backslash-escape = true
trim-last-separator = false

[tikv-importer]
 backend = &quot;local&quot;
 sorted-kv-dir = &quot;/kvdir/&quot;

disk-quota = &quot;1.5TiB&quot;

[mydumper]
data-source-dir = &quot;/csv_dir/&quot;
strict-format = false
no-schema = true

[tidb]
host = &quot;xxx&quot;
port = 3306
user = &quot;github_events&quot;
password = &quot;******&quot;

[lightning]
check-requirements = false
region-concurrency = 32
meta-schema-name = &quot;gharchive_meta&quot;
</code></pre><ol start="3"><li>Load the data into the TiDB cluster. </li></ol><pre><code class="language-bash">nohup tidb-lightning -config ./tidb-lightning.toml &gt; nohup.out
</code></pre><ol start="4"><li>Convert the unstructured <code>json file</code> provided by GH Archive into structured data. </li></ol><pre><code class="language-sql">gharchive_dev&gt; desc github_events;
+--------------------+--------------+------+-----+---------+-------+
| Field              | Type         | Null | Key | Default | Extra |
+--------------------+--------------+------+-----+---------+-------+
| id                 | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| type               | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| created_at         | datetime     | YES  | MUL | &lt;null&gt;  |       |
| repo_id            | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| repo_name          | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| actor_id           | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| actor_login        | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| actor_location     | varchar(255) | YES  |     | &lt;null&gt;  |       |
| language           | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| additions          | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| deletions          | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| action             | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| number             | int(11)      | YES  |     | &lt;null&gt;  |       |
| commit_id          | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| comment_id         | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| org_login          | varchar(255) | YES  | MUL | &lt;null&gt;  |       |
| org_id             | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| state              | varchar(255) | YES  |     | &lt;null&gt;  |       |
| closed_at          | datetime     | YES  | MUL | &lt;null&gt;  |       |
| comments           | int(11)      | YES  | MUL | &lt;null&gt;  |       |
| pr_merged_at       | datetime     | YES  | MUL | &lt;null&gt;  |       |
| pr_merged          | tinyint(1)   | YES  |     | &lt;null&gt;  |       |
| pr_changed_files   | int(11)      | YES  | MUL | &lt;null&gt;  |       |
| pr_review_comments | int(11)      | YES  | MUL | &lt;null&gt;  |       |
| pr_or_issue_id     | bigint(20)   | YES  | MUL | &lt;null&gt;  |       |
| event_day          | date         | YES  | MUL | &lt;null&gt;  |       |
| event_month        | date         | YES  | MUL | &lt;null&gt;  |       |
| author_association | varchar(255) | YES  |     | &lt;null&gt;  |       |
| event_year         | int(11)      | YES  | MUL | &lt;null&gt;  |       |
| push_size          | int(11)      | YES  |     | &lt;null&gt;  |       |
| push_distinct_size | int(11)      | YES  |     | &lt;null&gt;  |       |
+--------------------+--------------+------+-----+---------+-------+
</code></pre><ol start="5"><li>With structured data at hand, we can start to make further analysis with TiDB Cloud. Execute SQL commands to generate analytical results. For example, you can execute SQL commands below to output the top 10 most starred JavaScript framework repos in 2021.</li></ol><pre><code class="language-sql">  SELECT js.name, count(*) as stars 
    FROM github_events 
         JOIN js_framework_repos js ON js.id = github_events.repo_id 
   WHERE type = &#x27;WatchEvent&#x27; and event_year = 2021 
GROUP BY 1 
ORDER BY 2 DESC
   LIMIT 10;
+-------------------+-------+
| name              | stars |
+-------------------+-------+
| facebook/react    | 22830 |
| sveltejs/svelte   | 18573 |
| vuejs/vue         | 18015 |
| angular/angular   | 11037 |
| alpinejs/alpine   | 6993  |
| preactjs/preact   | 2965  |
| hotwired/stimulus | 1355  |
| marko-js/marko    | 1006  |
| neomjs/neo        | 826   |
| tastejs/todomvc   | 813   |
+-------------------+-------+
</code></pre><p>We have analyzed all the GitHub projects regarding databases, JavaScripe frameworks, programming languages, web frameworks, and low-code development tools, and provided valuable insights in 2021, in real time, and custom insights. If the repository you care about is not included here, you&#x27;re welcome to submit your PR <a href="https://github.com/hooopo/gharchive/tree/main/meta/repos">here</a>. If you want to gain more insights into other areas, you can try TiDB Cloud by yourselves with this <a href="https://ossinsight.io/blog/try-it-yourself/">10 minute tutorial</a>. </p><p>Below are the areas of GitHub projects we have analyzed. </p><pre><code class="language-sql">gharchive_dev&gt; show tables;
+-----------------------------+
| Tables_in_gharchive_dev     |
+-----------------------------+
| cn_repos                    |
| css_framework_repos         |
| db_repos                    |
| github_events               |
| js_framework_repos          |
| nocode_repos                |
| programming_language_repos  |
| static_site_generator_repos |
| web_framework_repos         |
+-----------------------------+
</code></pre>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[▶️  Use TiDB Cloud to Analyze GitHub Events in 10 Minutes]]></title>
        <id>/try-it-yourself</id>
        <link href="https://ossinsight.io/blog/try-it-yourself"/>
        <updated>2022-04-22T02:51:36.668Z</updated>
        <summary type="html"><![CDATA[TiDB is an open source distributed NewSQL database with horizontal scalability, high availability, and strong consistency. It can also deal with mixed OLTP and OLAP workloads at the same time by leveraging its hybrid transactional and analytical (HTAP) capability.]]></summary>
        <content type="html"><![CDATA[<p><a href="https://docs.pingcap.com/tidb/stable/overview">TiDB</a> is an open source distributed NewSQL database with horizontal scalability, high availability, and strong consistency. It can also deal with mixed OLTP and OLAP workloads at the same time by leveraging its hybrid transactional and analytical (HTAP) capability. </p><p><strong><a href="https://docs.pingcap.com/tidbcloud/public-preview">TiDB Cloud</a> is a fully-managed Database-as-a-Service (DBaaS)</strong> that brings everything great about TiDB to your cloud and lets you focus on your applications, not the complexities of your database. </p><p>In this tutorial, we will provide you with a piece of sample data of all GitHub events occurring on January 1, 2022, and walk you through on how to use TiDB Cloud to analyze this data in 10 minutes.  </p><h2>Sign up for a TiDB Cloud account (Free)</h2><ol><li>Click <a href="https://tidbcloud.com/signup">here</a> to sign up for a TiDB Cloud account free of charge. </li><li><a href="https://tidbcloud.com/?__hstc=86493575.35e9113e934f84907657eac98c81808d.1645076526216.1646808171218.1646883397864.20&amp;__hssc=86493575.6.1646883397864&amp;__hsfp=2077185778">Log in</a> to your account.</li></ol><h2>Create a TiDB Developer Tier cluster (Free)</h2><p>Once you register an account, you can create a free cluster with TiDB Developer Tier. </p><p>:::info
A cluster is a database to store data.
:::</p><ol><li>Click <strong>Get Started for Free</strong> and start to create a free cluster. </li></ol><p><img src="/img/try-it-yourself/dev-tier.png"/></p><ol start="2"><li>On the <strong>Create a Cluster</strong> page, set up your cluster name and root password.</li><li>Note that the cloud provider is AWS by default, and then select the <code>US-West-2 (Oregon)</code> region to create the cluster.</li><li>The cluster tier is S1.dev by default.</li><li>Click <strong>Submit</strong>.
Your TiDB Cloud cluster will be created in approximately 5 to 10 minutes.</li></ol><p>:::note
The Developer Tier is <strong>free</strong> for 1 year.
:::</p><h2>Import data to your TiDB Cloud cluster</h2><h3>Import the data</h3><p>Once your cluster is ready, you can start to import the sample data to your cluster. </p><p>:::info
We have merged the create database/table in the SQL files, so you don&#x27;t need to <code>create database/tables</code> by yourself.</p><p>If you want to know the table schema, you can check <code>desc gharchive_dev</code> later in the following step.
:::</p><ol><li>Click the <strong>Import</strong> button on the <strong>Active Clusters</strong> page and then go to the <strong>Data Import Task</strong> page. </li></ol><p><img src="/img/try-it-yourself/import.png"/></p><ol start="2"><li>Copy the values below and paste to the blanks of <strong>Bucket URL</strong> and <strong>Role-ARN</strong> respectively on the <strong>Data Import Task</strong> page.</li></ol><p><strong>Bucket URL</strong>:</p><pre><code>s3://tidbcloud-samples/gharchive/
</code></pre><p><strong>Role-ARN</strong>:</p><pre><code>arn:aws:iam::385595570414:role/import-sample-access
</code></pre><ol start="3"><li>Choose <strong>US West (Oregon)</strong> for your <strong>Bucket region</strong>;</li><li>Tick <strong>TiDB Dumpling</strong> for the <strong>Data Format</strong>. </li><li>Input your cluster password in the blank of Password on the <strong>Target Database</strong> section. </li></ol><p><img src="/img/try-it-yourself/fill.png"/></p><ol start="6"><li>After you fill in all the blanks on the <strong>Data Import Task</strong> page, click the <strong>Import</strong> button at the bottom of this page and wait for a few moments for the system to complete data importing. </li></ol><h3>Use the web shell to check if data is ready</h3><p>TiDB Cloud provides a web shell to connect the database online. </p><ol><li>Click the <strong>Exit</strong> button after you successfully import the data into your cluster. </li><li>Then, click the <strong>Connect</strong> button and the <strong>Connect to TiDB</strong> panel pops out. </li><li>Choose <strong>Web SQL Shell</strong> --&gt; <strong>Open SQL Shell</strong>. </li><li>Then input your cluster password as shown in the image below.</li></ol><p><img src="/img/try-it-yourself/web-shell.png"/></p><h3>Set column storage replica: TiFlash (Optional)</h3><p><a href="https://docs.pingcap.com/tidb/stable/tiflash-overview">TiFlash</a> is the key component that makes TiDB / TiDB Cloud an HTAP database and capable of dealing with OLTP and OLAP workloads at the same time. </p><p>Here, you can try the following SQL commands on TiDB Cloud to experience its real-time analytics with ease.</p><ol><li>Execute the SQL statements specified below </li></ol><pre><code class="language-sql">use gharchive_dev;
ALTER TABLE github_events SET TIFLASH REPLICA 1;
</code></pre><ol start="2"><li>Setting a TiFlash replica will take you some time, so you can use the following SQL statements to check if the procedure is done or not. </li></ol><pre><code class="language-sql">SELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = &#x27;gharchive_dev&#x27; and TABLE_NAME = &#x27;github_events&#x27;;
</code></pre><p>If the results you get are the same as follows, then it means the procedure is done. </p><pre><code class="language-sql">mysql&gt; SELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = &#x27;gharchive_dev&#x27; and TABLE_NAME = &#x27;github_events&#x27;;
+---------------+---------------+----------+---------------+-----------------+-----------+----------+
| TABLE_SCHEMA  | TABLE_NAME    | TABLE_ID | REPLICA_COUNT | LOCATION_LABELS | AVAILABLE | PROGRESS |
+---------------+---------------+----------+---------------+-----------------+-----------+----------+
| gharchive_dev | github_events |       68 |             1 |                 |         1 |        1 |
+---------------+---------------+----------+---------------+-----------------+-----------+----------+
1 row in set (0.27 sec)

mysql&gt;
</code></pre><h2>Analysis!</h2><p>After you finish all the steps above, you can start the analytical process. </p><p>:::tip
If you want to know the table schema, you can use <code>show create table tbl_name</code> to get that information.
:::</p><p>Because you have imported the sample data of all GitHub events occurred on the first hour of 2022 (from 2022-01-01 00:00:00 to 2022-01-01 00:59:59), you can start to make any queries based on that data by using SQL commands. </p><h3>How many events occurred in total?</h3><p>Execute the following SQL statement to query the total number of events. </p><pre><code class="language-sql">SELECT count(*) FROM github_events;
</code></pre><h3>Which repository gets the most stars?</h3><p>Execute the following statements to query the most starred repository. </p><pre><code class="language-sql">  SELECT repo_name, count(*) AS events_count
    FROM github_events
   WHERE type = &#x27;WatchEvent&#x27; /* Yes, `WatchEvent` means star */
GROUP BY 1
ORDER BY 2 DESC
   LIMIT 20;
</code></pre><h2>Mini Test</h2><p>Here is a small test for you to practice how to use TiDB Cloud to conduct analytics. </p><h3>Q: Who is the most active contributor except the robot accounts on the first hour of 2022?</h3><h3>Click for the answer. ⬇️</h3><details><summary>Click me to show answer</summary><p><pre><code class="language-sql">  SELECT actor_login, 
         count(*) AS events_count
    FROM github_events
   WHERE actor_login NOT LIKE &#x27;%bot%&#x27;
GROUP BY 1
ORDER BY 2 DESC 
   LIMIT 20
</code></pre></p></details><h2>Watch the video below for detailed information</h2><video width="100%" poster="/img/try-it-yourself/dev-tier.png" controls=""><source src="/video/github-demo-tidbcloud.mp4" type="video/mp4"/></video>]]></content>
    </entry>
</feed>